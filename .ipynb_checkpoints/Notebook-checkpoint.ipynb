{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import contextlib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1, L2, L1L2\n",
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use here the dataset provided by the authors we will create a helper function to extract the features that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seasons</th>\n",
       "      <th>game</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>idx</th>\n",
       "      <th>people</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'season': 1906.5, 'interaction': {'victim': ...</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>AT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'season': 1911.5, 'interaction': {'victim': ...</td>\n",
       "      <td>165</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             seasons  game  betrayal  idx  \\\n",
       "0  [{'season': 1906.5, 'interaction': {'victim': ...    74      True    0   \n",
       "1  [{'season': 1911.5, 'interaction': {'victim': ...   165     False    1   \n",
       "\n",
       "  people  \n",
       "0     AT  \n",
       "1     EG  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"diplomacy_data/diplomacy_data.json\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_support(entry):\n",
    "    \"\"\"\n",
    "    This function returns the last season of friendship. The code is inspired by the provided code from\n",
    "    the authors\n",
    "    \"\"\"\n",
    "    last_support = None\n",
    "    for season in entry[:-1]:\n",
    "        if 'support' in season['interaction'].values():\n",
    "            last_support = season['season']\n",
    "    return last_support\n",
    "\n",
    "def get_first_support(entry):\n",
    "    \"\"\"\n",
    "    This function returns the first season of friendship. \n",
    "    \"\"\"\n",
    "    for season in entry[:-1]:\n",
    "        if 'support' in season['interaction'].values():\n",
    "            return season['season']\n",
    "\n",
    "    return None\n",
    "\n",
    "def treat_msg_season(df):\n",
    "    \"\"\"\n",
    "    This function loops over the whole dataset and creates a dictionnary with the set of features for each season \n",
    "    with its associated boolean (betrayal or not )\n",
    "    \"\"\"\n",
    "    data_victim = {'features':[], 'betrayed':[]} # data of the (potential) victim \n",
    "    data_betrayer = {'features':[], 'betrayed':[]} # data of the (potential) betrayer\n",
    "    for i in range(len(df.seasons.values)):\n",
    "        entry = df['seasons'][i] # pick each entry\n",
    "        for j in range(len(entry)): # pick each season\n",
    "            season = entry[j]\n",
    "            tab_vi = []\n",
    "            tab_be = []\n",
    "            if season['season'] <= last_support(entry): # check if the season is below the last season of friendship\n",
    "                tab_vi.append(season['messages']['victim'])\n",
    "                tab_be.append(season['messages']['betrayer'])\n",
    "                if len(tab_be) != 0 and len(tab_vi) != 0: # keep only cases where both players have sent messages\n",
    "                    data_victim['features'].append(tab_vi)\n",
    "                    data_victim['betrayed'].append(df.betrayal.values[i])\n",
    "                    data_betrayer['features'].append(tab_be)   \n",
    "                    data_betrayer['betrayed'].append(df.betrayal.values[i])\n",
    "    return data_victim, data_betrayer\n",
    "\n",
    "data_victim, data_betrayer = treat_msg_season(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_unique_words(message):\n",
    "    words = message['frequent_words']\n",
    "    for _, value in message['lexicon_words'].items():\n",
    "        words += value\n",
    "    \n",
    "    return list(set(words))\n",
    "\n",
    "\n",
    "def collect_all_disc_words(message):\n",
    "    words = []\n",
    "    for _, value in message['lexicon_words'].items():\n",
    "        words += value\n",
    "    return words\n",
    "\n",
    "    \n",
    "def to_dict(message):\n",
    "    sentiment_positive = message['sentiment']['positive']\n",
    "    sentiment_neutral = message['sentiment']['neutral']\n",
    "    sentiment_negative = message['sentiment']['negative']\n",
    "    n_requests = message['n_requests']\n",
    "    frequent_words = message['frequent_words']\n",
    "    all_words = collect_all_unique_words(message)\n",
    "    disc_words = collect_all_disc_words(message)\n",
    "    n_disc_words = len(collect_all_disc_words(message))\n",
    "    n_words = message['n_words']\n",
    "    politeness = message['politeness']\n",
    "    n_sentences = message['n_sentences']\n",
    "    return {\"sentiment_positive\": sentiment_positive,\n",
    "           \"sentiment_neutral\": sentiment_neutral,\n",
    "           'sentiment_negative': sentiment_negative,\n",
    "           'n_requests': n_requests,\n",
    "           'frequent_words': frequent_words,\n",
    "           'n_words': n_words,\n",
    "           'politeness': politeness,\n",
    "           'n_sentences': n_sentences,\n",
    "            'n_disc_words': n_disc_words,\n",
    "            'disc_words': disc_words,\n",
    "           'all_words': all_words}\n",
    "\n",
    "\n",
    "def preprocessing(df):\n",
    "    result = []\n",
    "    for row in df.iterrows():\n",
    "        row = row[1]\n",
    "        betrayal = row['betrayal']\n",
    "        idx = row['idx']\n",
    "        for season in row['seasons']:\n",
    "            s = season['season']\n",
    "                \n",
    "            last_s = last_support(row['seasons'])+0.5 # the betrayal occurs one season after the last support\n",
    "            first_support = get_first_support(row['seasons'])\n",
    "            if s <= last_support(row['seasons']) and len(season['messages']['betrayer']) and len(season['messages']['victim']): # here we also have to consider the last season before betrayal\n",
    "                interaction_victim = season['interaction']['victim']\n",
    "                interaction_betrayer = season ['interaction']['betrayer']\n",
    "                for m_vic in season['messages']['victim']:\n",
    "                    data = to_dict(m_vic)\n",
    "                    data['role'] = 'victim'\n",
    "                    data['season'] = s\n",
    "                    data['betrayal'] = betrayal\n",
    "                    data['season_betrayal'] = last_s\n",
    "                    data['season_before_betrayal'] = (last_s-s)/0.5\n",
    "                    data['idx'] = idx\n",
    "                    data['friendship_length'] = (last_s-first_support) if first_support else 0\n",
    "                    result.append(data)\n",
    "                for m_bet in season['messages']['betrayer']:\n",
    "                    data = to_dict(m_bet)\n",
    "                    data['role'] = 'betrayer'\n",
    "                    data['season'] = s\n",
    "                    data['betrayal'] = betrayal\n",
    "                    data['season_betrayal'] = last_s\n",
    "                    data['season_before_betrayal'] = (last_s-s)/0.5\n",
    "                    data['friendship_length'] = (last_s-first_support) if first_support else 0\n",
    "                    data['idx'] = idx\n",
    "                    result.append(data)\n",
    "                            \n",
    "    return pd.DataFrame(result).set_index(['idx', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>n_disc_words</th>\n",
       "      <th>disc_words</th>\n",
       "      <th>all_words</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "      <th>friendship_length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1906.5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, bot, ,, ., take, unit, war, retreat, di...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>[just, war, prefer, really, light, retreat, in...</td>\n",
       "      <td>[me, in light of, take, to, unit, ., bot, mos,...</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[armies, north, the, armies, on, ., your, with...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.932326</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>[against, lose, even, loss, support, attack, g...</td>\n",
       "      <td>[we, germany, you, moves, attack, on, to, lose...</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[?, going, for, ser, balance, a, to, of, give,...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>[rather, light, would, lose, retreat, could, p...</td>\n",
       "      <td>[germany, balance, me, you, supply, in light o...</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>[only, he, alb, ., forced, italy's, is, be, .,...</td>\n",
       "      <td>313</td>\n",
       "      <td>0.957072</td>\n",
       "      <td>19</td>\n",
       "      <td>71</td>\n",
       "      <td>[so, so, before, while, before, still, as long...</td>\n",
       "      <td>[surely, alb, me, you, lack, to, s, (, fine, b...</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>[more, let, keep, we, side, we, don't, to, ., ...</td>\n",
       "      <td>146</td>\n",
       "      <td>0.832023</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>[before, still, move, ally, will, want, will, ...</td>\n",
       "      <td>[me, you, to, be, my, most, yet, out, retake, ...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "idx season                                                              \n",
       "0   1906.5                   0                  0                   2   \n",
       "    1906.5                   1                  1                   4   \n",
       "    1906.5                   1                  2                   1   \n",
       "    1906.5                   4                  2                  13   \n",
       "    1906.5                   1                  3                   5   \n",
       "\n",
       "            n_requests                                     frequent_words  \\\n",
       "idx season                                                                  \n",
       "0   1906.5           1  [just, bot, ,, ., take, unit, war, retreat, di...   \n",
       "    1906.5           2  [armies, north, the, armies, on, ., your, with...   \n",
       "    1906.5           2  [?, going, for, ser, balance, a, to, of, give,...   \n",
       "    1906.5           8  [only, he, alb, ., forced, italy's, is, be, .,...   \n",
       "    1906.5           7  [more, let, keep, we, side, we, don't, to, ., ...   \n",
       "\n",
       "            n_words  politeness  n_sentences  n_disc_words  \\\n",
       "idx season                                                   \n",
       "0   1906.5       35    0.367200            2             7   \n",
       "    1906.5       77    0.932326            6            16   \n",
       "    1906.5       55    0.983373            4            14   \n",
       "    1906.5      313    0.957072           19            71   \n",
       "    1906.5      146    0.832023            9            38   \n",
       "\n",
       "                                                   disc_words  \\\n",
       "idx season                                                      \n",
       "0   1906.5  [just, war, prefer, really, light, retreat, in...   \n",
       "    1906.5  [against, lose, even, loss, support, attack, g...   \n",
       "    1906.5  [rather, light, would, lose, retreat, could, p...   \n",
       "    1906.5  [so, so, before, while, before, still, as long...   \n",
       "    1906.5  [before, still, move, ally, will, want, will, ...   \n",
       "\n",
       "                                                    all_words      role  \\\n",
       "idx season                                                                \n",
       "0   1906.5  [me, in light of, take, to, unit, ., bot, mos,...    victim   \n",
       "    1906.5  [we, germany, you, moves, attack, on, to, lose...    victim   \n",
       "    1906.5  [germany, balance, me, you, supply, in light o...    victim   \n",
       "    1906.5  [surely, alb, me, you, lack, to, s, (, fine, b...    victim   \n",
       "    1906.5  [me, you, to, be, my, most, yet, out, retake, ...  betrayer   \n",
       "\n",
       "            betrayal  season_betrayal  season_before_betrayal  \\\n",
       "idx season                                                      \n",
       "0   1906.5      True           1909.5                     6.0   \n",
       "    1906.5      True           1909.5                     6.0   \n",
       "    1906.5      True           1909.5                     6.0   \n",
       "    1906.5      True           1909.5                     6.0   \n",
       "    1906.5      True           1909.5                     6.0   \n",
       "\n",
       "            friendship_length  \n",
       "idx season                     \n",
       "0   1906.5                3.0  \n",
       "    1906.5                3.0  \n",
       "    1906.5                3.0  \n",
       "    1906.5                3.0  \n",
       "    1906.5                3.0  "
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessing(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In each season, potential betrayers send in average 1.627498001598721, with a maximum of 38 messages\n",
      "In each season, potential victims send in average 1.515587529976019, with a maximum of 28 messages\n"
     ]
    }
   ],
   "source": [
    "def get_nb_msg(data):\n",
    "    \"\"\"\n",
    "    Get the mean number of messages sent per season\n",
    "    \"\"\"\n",
    "    tab = []\n",
    "    for features in data[\"features\"]:\n",
    "        tab.append(len(features[0]))\n",
    "    return tab\n",
    "\n",
    "print(\"In each season, potential betrayers send in average {}, with a maximum of {} messages\".format(np.mean(get_nb_msg(data_betrayer)), np.max(get_nb_msg(data_betrayer))))\n",
    "print(\"In each season, potential victims send in average {}, with a maximum of {} messages\".format(np.mean(get_nb_msg(data_victim)), np.max(get_nb_msg(data_victim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season_before_betrayal\n",
       "1.0    343\n",
       "2.0    360\n",
       "3.0    314\n",
       "4.0    277\n",
       "5.0    231\n",
       "6.0    156\n",
       "7.0     94\n",
       "8.0     78\n",
       "9.0     42\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"betrayal\"]==True) & (df[\"role\"] == \"betrayer\")].reset_index().groupby('season_before_betrayal').count()['idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 367 instances of 1 season before betrayal, 379 instances of 2 seasons before betrayal etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Imminent Betrayal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done similar to the authors in Section 5 to see if we can improve results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores we analyze for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def matthews_corr_coef(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TP, FP, FN, TN = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = np.sum(y_pred == y_true) / n\n",
    "    \n",
    "    mmc = matthews_corr_coef(y_true, y_pred)\n",
    "    \n",
    "    return {'f1': np.round(f1, decimals=3),\n",
    "            'mmc': np.round(mmc, decimals=6),\n",
    "            'acc': np.round(accuracy, decimals=3),\n",
    "            'precision': np.round(precision, decimals=3),\n",
    "            'recall': np.round(recall, decimals=3),\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'tn': tn,\n",
    "            'fn': fn\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping util for analyzing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "\n",
    "def confidence_interval(confidence, values):\n",
    "    \"\"\"Computes the confidence interval for the given set of values.\"\"\"\n",
    "    lower_p = (1.0 - confidence)/2\n",
    "    upper_p = 1 - (1.0 - confidence)/2\n",
    "    return np.percentile(values, lower_p * 100), np.percentile(values, upper_p * 100)\n",
    "\n",
    "def bootstrap_model_prediction(train_and_predict_fn, n_iterations, stratify, model_name):\n",
    "    \"\"\"Use to run the model for a number of iterations and get bootstrapped results.\"\"\"\n",
    "    \n",
    "    print('Training model {}..'.format(model_name))    \n",
    "    \n",
    "    f1_scores = []\n",
    "    mmc_scores = []\n",
    "    for i in range(n_iterations):\n",
    "        \n",
    "        # Split data before resampling\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42, shuffle=True)\n",
    "\n",
    "        # Resample train and test data, stratifying on y (resulting in equal number of 0 and 1 labels)\n",
    "        if stratify:\n",
    "            x_train, y_train = resample(x_train, y_train, replace=True, stratify=y_train)\n",
    "        else:\n",
    "            x_train, y_train = resample(x_train, y_train, replace=True)\n",
    "            \n",
    "        x_test, y_test = resample( x_test, y_test, replace=True)\n",
    "                \n",
    "        scores = train_and_predict_fn(x_train, x_test, y_train, y_test)\n",
    "        \n",
    "        # We store the f1 and mmc scores only to compare to authors' baseline\n",
    "        f1_scores.append(scores['f1'])\n",
    "        mmc_scores.append(scores['mmc'])\n",
    "    f1_score_low, f1_score_upper = confidence_interval(0.95, f1_scores)\n",
    "    mmc_score_low, mmc_score_upper = confidence_interval(0.95, np.nan_to_num(mmc_scores))\n",
    "    \n",
    "    f1_avg_score, f1_avg_score_err = np.mean(f1_scores),  sem(f1_scores)\n",
    "    mmc_avg_score, mmc_avg_score_err = np.mean(mmc_scores), sem(mmc_scores)\n",
    "    \n",
    "    print('Average F1-Score: %.3f' % f1_avg_score)\n",
    "    print('Average MMC-Score: %.3f' % mmc_avg_score)\n",
    "    print('F1-Score: 95%% confidence interval %.3f and %.3f' % (f1_score_low, f1_score_upper))\n",
    "    print('Matthews Corr Coef: 95%% confidence interval %.3f and %.3f' % (mmc_score_low, mmc_score_upper))\n",
    "    \n",
    "    return [ f1_avg_score, f1_avg_score_err], [mmc_avg_score, mmc_avg_score_err]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To address imbalance, compute the weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "def get_class_weights(Y):\n",
    "    weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "    class_weights = {0: weights[0], 1: weights[1]}\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_disc_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>friendship_length</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th>role</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">1906.5</th>\n",
       "      <th>betrayer</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>489</td>\n",
       "      <td>120</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>25</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>victim</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>480</td>\n",
       "      <td>108</td>\n",
       "      <td>0.809993</td>\n",
       "      <td>31</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1907.0</th>\n",
       "      <th>betrayer</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>280</td>\n",
       "      <td>46</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>16</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>victim</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>93</td>\n",
       "      <td>15</td>\n",
       "      <td>0.785508</td>\n",
       "      <td>9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907.5</th>\n",
       "      <th>betrayer</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>333</td>\n",
       "      <td>87</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>13</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     sentiment_positive  sentiment_neutral  \\\n",
       "idx season role                                              \n",
       "0   1906.5 betrayer            1.333333           1.333333   \n",
       "           victim              1.500000           1.250000   \n",
       "    1907.0 betrayer            0.142857           0.857143   \n",
       "           victim              1.333333           0.666667   \n",
       "    1907.5 betrayer            2.000000           2.500000   \n",
       "\n",
       "                     sentiment_negative  n_requests  n_words  n_disc_words  \\\n",
       "idx season role                                                              \n",
       "0   1906.5 betrayer            1.500000    3.666667      489           120   \n",
       "           victim              5.000000    3.250000      480           108   \n",
       "    1907.0 betrayer            1.285714    1.285714      280            46   \n",
       "           victim              1.000000    0.666667       93            15   \n",
       "    1907.5 betrayer            2.000000    5.500000      333            87   \n",
       "\n",
       "                     politeness  n_sentences  friendship_length  \\\n",
       "idx season role                                                   \n",
       "0   1906.5 betrayer    0.803328           25                3.0   \n",
       "           victim      0.809993           31                3.0   \n",
       "    1907.0 betrayer    0.560083           16                3.0   \n",
       "           victim      0.785508            9                3.0   \n",
       "    1907.5 betrayer    0.982703           13                3.0   \n",
       "\n",
       "                     season_before_betrayal  \n",
       "idx season role                              \n",
       "0   1906.5 betrayer                     6.0  \n",
       "           victim                       6.0  \n",
       "    1907.0 betrayer                     5.0  \n",
       "           victim                       5.0  \n",
       "    1907.5 betrayer                     4.0  "
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_data = df.copy()\n",
    "\n",
    "# Consider only the betrayals relationships\n",
    "features_data = features_data[features_data['betrayal'] == True]\n",
    "\n",
    "# Drop words as we cannot consider as features here and betrayal infos\n",
    "features_data = features_data.drop(columns=['frequent_words', 'all_words', 'season_betrayal', 'betrayal'])\n",
    "\n",
    "aggreagted_features_per_season = features_data.groupby(['idx', 'season', 'role'], as_index=True).aggregate({\n",
    "    'sentiment_positive': 'mean',\n",
    "    'sentiment_neutral': 'mean',\n",
    "    'sentiment_negative': 'mean',\n",
    "    'n_requests': 'mean',\n",
    "    'n_words': 'sum',\n",
    "    'n_disc_words': 'sum',\n",
    "    'politeness': 'mean',\n",
    "    'n_sentences': 'sum',\n",
    "    'friendship_length': 'min', # Same\n",
    "    'season_before_betrayal': 'min' # Same\n",
    "})\n",
    "\n",
    "aggreagted_features_per_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_per_season = aggreagted_features_per_season.reset_index()\n",
    "\n",
    "# Add difference between politeness score in victim and betrayer\n",
    "def delta_politeness(x):\n",
    "    return {'delta_role_politeness': (x[x['role'] == 'betrayer']['politeness'].values - x[x['role'] == 'victim']['politeness'].values).item()}\n",
    "delta_politeness_role = features_per_season.groupby(by=['idx', 'season']).apply(lambda x: pd.Series(delta_politeness(x))).reset_index()\n",
    "\n",
    "\n",
    "def get_politeness_season(features_per_season):\n",
    "    data_time = features_per_season.groupby(['idx']).apply(lambda x: pd.Series({\n",
    "        'delta_time': x['politeness'].values - np.append(x['politeness'].values[0], x['politeness'].values[:-1])\n",
    "}))\n",
    "    delta_time_list = list()\n",
    "    for dt in data_time['delta_time']:\n",
    "        delta_time_list = np.append(delta_time_list, dt)\n",
    "    return delta_time_list\n",
    "features_per_season['delta_time_politeness'] = get_politeness_season(features_per_season)\n",
    "\n",
    "# Consider just betrayer data\n",
    "input_features = features_per_season[features_per_season['role'] == 'betrayer'].set_index(['idx', 'season']).join(delta_politeness_role.set_index(['idx', 'season'])).reset_index()\n",
    "\n",
    "X = input_features[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',  'n_requests', 'n_words', 'politeness', 'n_sentences', 'delta_role_politeness', 'delta_time_politeness', 'friendship_length']]\n",
    "Y = (input_features['season_before_betrayal'] == 1.0).values.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_f1_scores = []\n",
    "all_models_mmc_scores = []\n",
    "all_models_names = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model Logistic Regression (Nor)..\n",
      "Average F1-Score: 0.465\n",
      "Average MMC-Score: 0.288\n",
      "F1-Score: 95% confidence interval 0.173 and 0.704\n",
      "Matthews Corr Coef: 95% confidence interval -0.062 and 0.601\n",
      "\n",
      "Training model Logistic Regression (Nor)..\n",
      "Average F1-Score: 0.510\n",
      "Average MMC-Score: 0.343\n",
      "F1-Score: 95% confidence interval 0.351 and 0.705\n",
      "Matthews Corr Coef: 95% confidence interval 0.079 and 0.642\n"
     ]
    }
   ],
   "source": [
    "def train_and_predict_logistic_regressionl_normalized(x_train, x_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, class_weight=get_class_weights(Y)).fit(x_train, y_train)                                       \n",
    "    y_pred = clf.predict(x_test)\n",
    "    return evaluate_model(y_test, y_pred)\n",
    "\n",
    "# Without\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_logistic_regressionl_normalized, 20, stratify=False, model_name=\"Logistic Regression (Nor)\")\n",
    "\n",
    "print(\"\")\n",
    "# With stratify\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_logistic_regressionl_normalized, 20, stratify=True, model_name=\"Logistic Regression (Nor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_f1_scores.append(f1_output)\n",
    "all_models_mmc_scores.append(mmc_output)\n",
    "all_models_names.append(\"Logistic Regression (Nor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIX THIS: An intuition for finding the best model / configuration of features for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def log_reg_pipeline(features, normalize_=True, print_=True):\n",
    "    x = X[features].values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, Y, test_size=0.10, random_state=42)\n",
    "    if normalize_:\n",
    "        x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "        clf = LogisticRegression(random_state=0).fit(x_train_nor, y_train)\n",
    "        score = clf.score(x_test_nor, y_test)\n",
    "        if print_:\n",
    "            print(\"score on the test set : {}\".format(score))\n",
    "        return clf, score\n",
    "    else:\n",
    "        clf = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
    "        score = clf.score(x_test, y_test)\n",
    "        if print_:\n",
    "            print(\"score on the test set : {}\".format(score))\n",
    "        return clf, score\n",
    "\n",
    "features_test = [\n",
    "            ['politeness'],\n",
    "            ['n_words', 'politeness', 'delta_role_politeness', 'delta_time_politeness'],\n",
    "            ['n_words', 'politeness'],\n",
    "            ['n_requests', 'n_words', 'politeness'],\n",
    "            ['sentiment_positive', 'n_requests', 'n_words', 'politeness'],\n",
    "            ['sentiment_positive', 'sentiment_neutral', 'n_requests', 'n_words', 'politeness'],\n",
    "           ['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', 'politeness'],\n",
    "]\n",
    "\n",
    "ax1 = []\n",
    "ax2 = []\n",
    "ax3 = []\n",
    "\n",
    "for feature in features_test:\n",
    "    _, score = log_reg_pipeline(feature, False, False)\n",
    "    _, score_nor = log_reg_pipeline(feature, True, False)\n",
    "    ax1.append(len(feature))\n",
    "    ax2.append(score)\n",
    "    ax3.append(score_nor)\n",
    "    \n",
    "_ = plt.plot(ax1, ax2)\n",
    "_ = plt.plot(ax1, ax3)\n",
    "_ = plt.legend(['not normalized', 'normalized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model Random Forest..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-Score: 0.089\n",
      "Average MMC-Score: nan\n",
      "F1-Score: 95% confidence interval 0.000 and 0.400\n",
      "Matthews Corr Coef: 95% confidence interval -0.088 and 0.441\n",
      "\n",
      "Training model Random Forest..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n",
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average F1-Score: 0.052\n",
      "Average MMC-Score: nan\n",
      "F1-Score: 95% confidence interval 0.000 and 0.235\n",
      "Matthews Corr Coef: 95% confidence interval -0.057 and 0.299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "features = ['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',  'n_requests', 'n_words', 'politeness', 'n_sentences', 'delta_role_politeness', 'delta_time_politeness', 'friendship_length']\n",
    "X = input_features[features]\n",
    "Y = (input_features['season_before_betrayal'] == 1.0).values.astype(np.int)\n",
    "\n",
    "def train_and_predict_random_forest(x_train, x_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=1000, class_weight=get_class_weights(Y))\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return evaluate_model(y_test, y_pred)\n",
    "\n",
    "# Without\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_random_forest, 5, stratify=False, model_name=\"Random Forest\")\n",
    "\n",
    "print(\"\")\n",
    "# With stratify\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_random_forest, 5, stratify=True, model_name=\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_f1_scores.append(f1_output)\n",
    "all_models_mmc_scores.append(mmc_output)\n",
    "all_models_names.append(\"Random Forest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.776,\n",
       " 'f1': 0.118,\n",
       " 'fn': 14,\n",
       " 'fp': 1,\n",
       " 'mmc': 0.116194,\n",
       " 'precision': 0.5,\n",
       " 'recall': 0.067,\n",
       " 'tn': 51,\n",
       " 'tp': 1}"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train_rf_and_plot_importance(x_train, x_test, y_train, y_test ):\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators=1000)\n",
    "    clf.fit(x_train, y_train)\n",
    "\n",
    "    importances = np.array(clf.feature_importances_)\n",
    "    indices = np.argsort(importances)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), np.array(features)[indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "train_rf_and_plot_importance(x_train, x_test, y_train, y_test)\n",
    "train_and_predict_random_forest(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.362, 'mmc': 0.08248, 'acc': 68.368, 'precision': 0.233, 'recall': 0.474, 'tp': 1953, 'fp': 6426, 'tn': 7140, 'fn': 2170}\n",
      "Training model Feedforward..\n",
      "Average F1-Score: 0.454\n",
      "Average MMC-Score: 0.287\n",
      "F1-Score: 95% confidence interval 0.352 and 0.631\n",
      "Matthews Corr Coef: 95% confidence interval 0.145 and 0.550\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "FEATURES_NUM = X.shape[-1]\n",
    "\n",
    "def get_feed_forward_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=512, activation='tanh', kernel_regularizer=L2(1e-5), bias_regularizer=L2(1e-5), input_shape=(FEATURES_NUM,)))\n",
    "    model.add(Dense(units=256))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy')    \n",
    "    return model\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def local_seed(seed):\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        np.random.set_state(state)\n",
    "        tf.random.set_seed(np.random.randint(1000))\n",
    "        \n",
    "def train_and_predict_feed_forward_model(x_train, x_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n",
    "\n",
    "    class_weights = get_class_weights(Y)\n",
    "    with local_seed(10):\n",
    "        model = get_feed_forward_model()\n",
    "        result = model.fit(\n",
    "            x_train, \n",
    "            y_train, \n",
    "            batch_size = BATCH_SIZE, \n",
    "            epochs=10,\n",
    "            callbacks=[es],\n",
    "            validation_data=(x_test, y_test),\n",
    "            class_weight=class_weights, verbose=0)\n",
    "\n",
    "    y_pred = model.predict_step(x_test).numpy()\n",
    "    y_pred_bool = y_pred > 0.5\n",
    "    return evaluate_model(y_test, y_pred_bool)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 10, test_size=0.2)\n",
    "print(train_and_predict_feed_forward_model(x_train, x_test, y_train, y_test))\n",
    "\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_feed_forward_model, 20, stratify=True, model_name=\"Feedforward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_f1_scores.append(f1_output)\n",
    "all_models_mmc_scores.append(mmc_output)\n",
    "all_models_names.append(\"Feed Forward NN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_f1_scores.append([0.31, 0])\n",
    "all_models_mmc_scores.append([0.15, 0])\n",
    "all_models_names.append('Authors baseline')\n",
    "\n",
    "f1_scores  = np.array(all_models_f1_scores)\n",
    "mmc_scores = np.array(all_models_mmc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(all_models_names, f1_scores[:, 0], xerr=f1_scores[:,1], capsize=4)\n",
    "plt.title(\"Score\")\n",
    "plt.xlabel(\"f1\".capitalize())\n",
    "plt.ylabel(\"Model\")\n",
    "plt.show()\n",
    "\n",
    "plt.barh(all_models_names, mmc_scores[:, 0], xerr=mmc_scores[:,1], capsize=4)\n",
    "plt.title(\"Score\")\n",
    "plt.xlabel(\"mmc\".capitalize())\n",
    "plt.ylabel(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/ina/.local/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: joblib in /home/ina/.local/lib/python3.7/site-packages (from nltk) (0.14.1)\n",
      "Requirement already satisfied: tqdm in /home/ina/.local/lib/python3.7/site-packages (from nltk) (4.38.0)\n",
      "Requirement already satisfied: click in /home/ina/.local/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/ina/.local/lib/python3.7/site-packages (from nltk) (2020.11.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ina/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = df.copy()\n",
    "\n",
    "# Now train just for the betrayer role for changes, although we could add data for both?\n",
    "features_data = features_data[features_data['role'] == 'betrayer']\n",
    "features_data = features_data[features_data['betrayal'] == True]\n",
    "\n",
    "features_data = features_data.drop(columns=['frequent_words', 'season_betrayal', 'role', 'betrayal'])\n",
    "\n",
    "aggreagted_features_per_season = features_data.groupby(['idx', 'season'], as_index=False).aggregate({\n",
    "    'sentiment_positive': 'mean',\n",
    "    'sentiment_neutral': 'mean',\n",
    "    'sentiment_negative': 'mean',\n",
    "    'n_requests': 'mean',\n",
    "    'n_words': 'sum',\n",
    "    'politeness': 'mean',\n",
    "    'n_sentences': 'sum',\n",
    "    'season_before_betrayal': 'min', # equal for all anyway\n",
    "    'all_words': 'sum'\n",
    "})\n",
    "\n",
    "X = aggreagted_features_per_season[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',\n",
    "       'n_requests', 'n_words', 'politeness', 'n_sentences', 'all_words']]\n",
    "Y = (aggreagted_features_per_season['season_before_betrayal'] == 1.0).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>489</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>25</td>\n",
       "      <td>[prevent, this, before, one, but, ,, support, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>280</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>16</td>\n",
       "      <td>[on, turn, other, ?, are, of, one, new, like, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>333</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>13</td>\n",
       "      <td>[then, this, suggestions, one, but, ,, support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>449</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>24</td>\n",
       "      <td>[hope, ., turkey, next, out, work, a, let's, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>0.899161</td>\n",
       "      <td>6</td>\n",
       "      <td>[?, suggestions, some, hello, but, confused, ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_positive  sentiment_neutral  sentiment_negative  n_requests  \\\n",
       "0            1.333333           1.333333            1.500000    3.666667   \n",
       "1            0.142857           0.857143            1.285714    1.285714   \n",
       "2            2.000000           2.500000            2.000000    5.500000   \n",
       "3            1.800000           0.800000            2.200000    3.200000   \n",
       "4            1.000000           1.000000            1.000000    2.000000   \n",
       "\n",
       "   n_words  politeness  n_sentences  \\\n",
       "0      489    0.803328           25   \n",
       "1      280    0.560083           16   \n",
       "2      333    0.982703           13   \n",
       "3      449    0.748802           24   \n",
       "4       78    0.899161            6   \n",
       "\n",
       "                                           all_words  \n",
       "0  [prevent, this, before, one, but, ,, support, ...  \n",
       "1  [on, turn, other, ?, are, of, one, new, like, ...  \n",
       "2  [then, this, suggestions, one, but, ,, support...  \n",
       "3  [hope, ., turkey, next, out, work, a, let's, g...  \n",
       "4  [?, suggestions, some, hello, but, confused, ,...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prevent', 'this', 'before', 'one', 'but', ',', 'support', 'want', 'lose', 'enemy', 'ally', 'losing', 'an', 'a', 'my', 'we', 'crazy', 'saying', 'side', 'stp', 'our', \"didn't\", 'it', 'hand', 'up', 'something', 'still', 'if', 'will', 'decide', 'fall', 'am', 'you', 'retake', 'ground', 'the', 'might', 'with', 'by', 'yet', 'in', \"don't\", 'to', 'have', 'from', 'of', 'until', 'army', 'bounce', 'i']\n"
     ]
    }
   ],
   "source": [
    "# Words combined look like this\n",
    "print(X['all_words'][0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# We notice multiple stop words need to be removed, as well as contractions\n",
    "def clean_words(X):\n",
    "    new_words = []\n",
    "    for word in X:\n",
    "        # We consider contractions not very valuable, as well as very short words/individual letters\n",
    "        if len(word) <= 2 or '\\'' in word:\n",
    "            continue\n",
    "        \n",
    "        # Removing stop words\n",
    "        if word in stopwords.words('english'):\n",
    "            continue\n",
    "        \n",
    "        new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "X['all_words'] = X['all_words'].map(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words per season 938\n",
      "Mean number of words per season 81.11915535444948\n",
      "Min number of words per season 0\n",
      "Number of unique words are 2146\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_words_per_season = [len(X['all_words'][i]) for i in range(X.shape[0])]\n",
    "print('Max number of words per season', np.max(total_words_per_season))\n",
    "print('Mean number of words per season', np.mean(total_words_per_season))\n",
    "print('Min number of words per season', np.min(total_words_per_season))\n",
    "\n",
    "all_words = []\n",
    "for idx in range(X.shape[0]):\n",
    "    all_words = all_words + X['all_words'][idx]\n",
    "print('Number of unique words are', len(set(all_words)))\n",
    "\n",
    "plt.plot(total_words_per_season)\n",
    "plt.show()\n",
    "\n",
    "# Based on the following, we decide to pad/truncate to max 200 words and use an embedding matrix of 2140 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings containing 1193514 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>489</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>25</td>\n",
       "      <td>[1342, 1486, 552, 1077, 1080, 1773, 826, 19, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>280</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>16</td>\n",
       "      <td>[740, 1486, 1238, 76, 438, 587, 1661, 1631, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>333</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>13</td>\n",
       "      <td>[1202, 1486, 552, 816, 1080, 700, 1782, 709, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>449</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>24</td>\n",
       "      <td>[1420, 1290, 265, 1063, 624, 818, 460, 2114, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>0.899161</td>\n",
       "      <td>6</td>\n",
       "      <td>[1202, 545, 1620, 1835, 1431, 996, 1468, 1290,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_positive  sentiment_neutral  sentiment_negative  n_requests  \\\n",
       "0            1.333333           1.333333            1.500000    3.666667   \n",
       "1            0.142857           0.857143            1.285714    1.285714   \n",
       "2            2.000000           2.500000            2.000000    5.500000   \n",
       "3            1.800000           0.800000            2.200000    3.200000   \n",
       "4            1.000000           1.000000            1.000000    2.000000   \n",
       "\n",
       "   n_words  politeness  n_sentences  \\\n",
       "0      489    0.803328           25   \n",
       "1      280    0.560083           16   \n",
       "2      333    0.982703           13   \n",
       "3      449    0.748802           24   \n",
       "4       78    0.899161            6   \n",
       "\n",
       "                                           all_words  \n",
       "0  [1342, 1486, 552, 1077, 1080, 1773, 826, 19, 1...  \n",
       "1  [740, 1486, 1238, 76, 438, 587, 1661, 1631, 14...  \n",
       "2  [1202, 1486, 552, 816, 1080, 700, 1782, 709, 1...  \n",
       "3  [1420, 1290, 265, 1063, 624, 818, 460, 2114, 1...  \n",
       "4  [1202, 545, 1620, 1835, 1431, 996, 1468, 1290,...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 25\n",
    "MAX_WORDS = 2147\n",
    "MAX_LEN = 200\n",
    "\n",
    "def read_low_dim_glove_embeddings(filename):\n",
    "    embeddings = {}\n",
    "    \n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embeddings[word] = np.asarray(values[1:], dtype='float32')\n",
    "    f.close()\n",
    "    \n",
    "    print('Embeddings containing %s word vectors.' % len(embeddings))\n",
    "    return embeddings\n",
    "\n",
    "embeddings = read_low_dim_glove_embeddings('glove.twitter.27B.25d.txt')\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "\n",
    "word_to_idx = {}\n",
    "idx = 0\n",
    "for word in list(set(all_words)):\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "        word_to_idx[word] = idx\n",
    "    idx = idx + 1\n",
    "\n",
    "def convert_word_to_id(words):\n",
    "    values = [word_to_idx[w] for w in words if w in word_to_idx]\n",
    "    \n",
    "    return values\n",
    "\n",
    "X['all_words'] = X['all_words'].map(convert_word_to_id)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 25)      53675       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           23040       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           lstm[0][0]                       \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           4128        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 81,388\n",
      "Trainable params: 27,713\n",
      "Non-trainable params: 53,675\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_submodel():\n",
    "    inputs = tf.keras.layers.Input(shape=(MAX_LEN))\n",
    "    embedding = tf.keras.layers.Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    lstm = tf.keras.layers.LSTM(64, activation='relu', kernel_regularizer=L2(1e-5), bias_regularizer=L2(1e-5))(embedding)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=lstm)\n",
    "    return model\n",
    "\n",
    "def get_numerical_submodel():\n",
    "    inputs = tf.keras.layers.Input(shape=(7))\n",
    "    dense = tf.keras.layers.Dense(64, kernel_regularizer=L2(1e-5), bias_regularizer=L2(1e-5))(inputs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=dense)\n",
    "    return model\n",
    "\n",
    "def get_combined_model():\n",
    "    model_embeddings = get_embedding_submodel()\n",
    "    model_numerical = get_numerical_submodel()    \n",
    "    combined = tf.keras.layers.concatenate([model_embeddings.output, model_numerical.output])\n",
    "    dense = tf.keras.layers.Dense(32, activation='linear')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(0.4)(combined)\n",
    "    final_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[model_embeddings.input, model_numerical.input], outputs=final_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0005))\n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_combined_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model Fusion model..\n",
      "F1-Score: 95% confidence interval 0.086 and 0.388\n",
      "Matthews Corr Coef: 95% confidence interval -0.258 and 0.117\n",
      "Average f1 0.2699\n",
      "Average mmc -0.050140599999999994\n"
     ]
    }
   ],
   "source": [
    "def get_numerical_model_x_inputs(x_train, x_test):\n",
    "    # Inputs for the numerical model\n",
    "    numerical_model_features = ['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',\n",
    "           'n_requests', 'n_words', 'politeness', 'n_sentences']\n",
    "    x_train = x_train[numerical_model_features]\n",
    "    x_test = x_test[numerical_model_features]\n",
    "\n",
    "    # Standardization of numerical features\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    return x_train, x_test\n",
    "\n",
    "def get_embeddings_model_x_inputs(x_train, x_test):\n",
    "    x_train = x_train['all_words']\n",
    "    x_test = x_test['all_words']\n",
    "    \n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(list(x_train), maxlen=MAX_LEN, padding='post')\n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(list(x_test), maxlen=MAX_LEN, padding='post')\n",
    "    return x_train, x_test\n",
    "\n",
    "def train_and_predict_fusion_model(x_train, x_test, y_train, y_test):\n",
    "    x_train_1, x_test_1 = get_embeddings_model_x_inputs(x_train, x_test)\n",
    "    x_train_2, x_test_2 = get_numerical_model_x_inputs(x_train, x_test)\n",
    "\n",
    "    with local_seed(10):\n",
    "        tf.keras.backend.clear_session()\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n",
    "\n",
    "        model = get_combined_model()\n",
    "        result = model.fit({'input_1': x_train_1, 'input_2': x_train_2},\n",
    "            y_train, \n",
    "            batch_size = 64, \n",
    "            epochs=10,\n",
    "            callbacks=[es],\n",
    "            validation_data=({'input_1': x_test_1, 'input_2': x_test_2}, y_test),\n",
    "            class_weight=class_weights, verbose=0)\n",
    "\n",
    "    y_pred = model.predict_step({'input_1': x_test_1, 'input_2': x_test_2}).numpy()\n",
    "    y_pred_bool = y_pred > 0.5\n",
    "    return evaluate_model(y_test, y_pred_bool)\n",
    "\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_fusion_model, 20, stratify=True, model_name=\"Fusion model\")\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 10, test_size=0.2, shuffle=True)\n",
    "# train_and_predict_fusion_model(x_train, x_test, y_train, y_test)\n",
    "print('Average f1', f1_output[0])\n",
    "print('Average mmc', mmc_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model Fusion model..\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 153ms/step - loss: 0.6972 - val_loss: 0.7208\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6963 - val_loss: 0.6788\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 0.6922 - val_loss: 0.6627\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.6777 - val_loss: 0.6738\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 127ms/step - loss: 0.6863 - val_loss: 0.6812\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6929 - val_loss: 0.6886\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6807 - val_loss: 0.6863\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.6858 - val_loss: 0.6876\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6834 - val_loss: 0.6834\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6833 - val_loss: 0.6815\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6790 - val_loss: 0.6762\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6772 - val_loss: 0.6751\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6800 - val_loss: 0.6817\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6738 - val_loss: 0.6783\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.6768 - val_loss: 0.6784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.6697 - val_loss: 0.6766\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6695 - val_loss: 0.6239\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6725 - val_loss: 0.6324\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6538 - val_loss: 0.6473\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6485 - val_loss: 0.6554\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6732 - val_loss: 0.6529\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6621 - val_loss: 0.6550\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6559 - val_loss: 0.6522\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6550 - val_loss: 0.6472\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6489 - val_loss: 0.6502\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6493 - val_loss: 0.6546\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6423 - val_loss: 0.6595\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6504 - val_loss: 0.6697\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6506 - val_loss: 0.6661\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6476 - val_loss: 0.6616\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.7005 - val_loss: 0.7101\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6657 - val_loss: 0.6785\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6530 - val_loss: 0.6620\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6652 - val_loss: 0.6559\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6521 - val_loss: 0.6560\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6473 - val_loss: 0.6545\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6615 - val_loss: 0.6565\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6626 - val_loss: 0.6579\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6544 - val_loss: 0.6555\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6490 - val_loss: 0.6526\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6437 - val_loss: 0.6470\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6488 - val_loss: 0.6441\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.6400 - val_loss: 0.6344\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6515 - val_loss: 0.6586\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6498 - val_loss: 0.6648\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 183ms/step - loss: 0.7050 - val_loss: 0.7132\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.6999 - val_loss: 0.6901\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7043 - val_loss: 0.6914\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6967 - val_loss: 0.6898\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7009 - val_loss: 0.6887\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 11291.3721 - val_loss: 0.6969\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6788 - val_loss: 0.7014\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6830 - val_loss: 0.7030\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6902 - val_loss: 0.7038\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6901 - val_loss: 0.7037\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6887 - val_loss: 0.7036\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6948 - val_loss: 0.7029\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6888 - val_loss: 0.7024\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6896 - val_loss: 0.7014\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6819 - val_loss: 0.7005\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7019 - val_loss: 0.7208\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6876 - val_loss: 0.6672\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.7037 - val_loss: 0.6584\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6924 - val_loss: 0.6644\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6809 - val_loss: 0.6664\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.6898 - val_loss: 0.6683\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6816 - val_loss: 0.6586\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6674 - val_loss: 0.6572\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.6780 - val_loss: 0.6552\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6744 - val_loss: 0.6544\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.6675 - val_loss: 0.6545\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.6598 - val_loss: 0.6495\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6756 - val_loss: 0.6450\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 0.6671 - val_loss: 0.6498\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 214014.9062 - val_loss: 0.6751\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 148ms/step - loss: 0.6968 - val_loss: 0.7179\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.6861 - val_loss: 0.6847\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.6733 - val_loss: 0.6802\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 114ms/step - loss: 0.6750 - val_loss: 0.6794\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6897 - val_loss: 0.6803\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6782 - val_loss: 0.6765\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6726 - val_loss: 0.6655\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6687 - val_loss: 0.6559\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 76ms/step - loss: 0.6627 - val_loss: 0.6515\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6613 - val_loss: 0.6528\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.6599 - val_loss: 0.6534\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 136ms/step - loss: 0.6617 - val_loss: 0.6556\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.6662 - val_loss: 0.6632\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.6606 - val_loss: 0.6708\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6555 - val_loss: 62103977984.0000\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 138ms/step - loss: 0.7463 - val_loss: 0.7515\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.7293 - val_loss: 0.7205\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7273 - val_loss: 0.7148\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.7151 - val_loss: 0.7234\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7199 - val_loss: 0.7222\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7361 - val_loss: 0.7300\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7166 - val_loss: 0.7364\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7220 - val_loss: 0.7405\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7108 - val_loss: 0.7406\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7145 - val_loss: 0.7372\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 83ms/step - loss: 0.7251 - val_loss: 0.7274\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.7130 - val_loss: 0.7334\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 213878.0938 - val_loss: 1087561.1250\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 2622420156416.0000 - val_loss: 1029873205248.0000\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 23173550571520.0000 - val_loss: 410129170432.0000\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 197ms/step - loss: 0.7129 - val_loss: 0.7474\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7141 - val_loss: 0.7129\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6891 - val_loss: 0.6811\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.7025 - val_loss: 0.6753\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.6984 - val_loss: 0.6739\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6986 - val_loss: 0.6839\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6900 - val_loss: 0.6857\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.6968 - val_loss: 0.6763\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6883 - val_loss: 0.6739\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6922 - val_loss: 0.6810\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6781 - val_loss: 0.6823\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 46266.0938 - val_loss: 0.6733\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 87.3646 - val_loss: 0.7164\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.6909 - val_loss: 0.7302\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.6916 - val_loss: 0.7363\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7278 - val_loss: 0.7302\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7267 - val_loss: 0.7078\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7260 - val_loss: 0.7135\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7202 - val_loss: 0.7229\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7144 - val_loss: 0.7252\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7188 - val_loss: 0.7292\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7125 - val_loss: 0.7224\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7099 - val_loss: 0.7177\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7046 - val_loss: 0.7190\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7080 - val_loss: 0.7163\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6953 - val_loss: 0.7179\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7043 - val_loss: 0.7212\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7074 - val_loss: 0.7225\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.6987 - val_loss: 0.7221\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.7018 - val_loss: 0.7249\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.7149 - val_loss: 0.7136\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6971 - val_loss: 0.6920\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6941 - val_loss: 0.6898\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7016 - val_loss: 0.6984\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7053 - val_loss: 0.7074\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.6834 - val_loss: 0.7074\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6922 - val_loss: 0.6993\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.6787 - val_loss: 0.6991\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6946 - val_loss: 0.6975\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 0.6931 - val_loss: 0.6970\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.6833 - val_loss: 0.6942\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.6921 - val_loss: 0.6925\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.6874 - val_loss: 0.6951\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.6907 - val_loss: 0.6932\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.6874 - val_loss: 0.6907\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7095 - val_loss: 0.7074\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.6931 - val_loss: 0.6743\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.7006 - val_loss: 0.6568\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.6839 - val_loss: 0.6707\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6750 - val_loss: 0.6808\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6792 - val_loss: 0.6800\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6768 - val_loss: 0.6808\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6748 - val_loss: 0.6815\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6651 - val_loss: 0.6808\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6656 - val_loss: 0.6811\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.6746 - val_loss: 0.6803\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6788 - val_loss: 0.6788\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.6679 - val_loss: 0.6837\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.6658 - val_loss: 0.6869\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6600 - val_loss: 6654.4868\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7505 - val_loss: 0.7950\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.7477 - val_loss: 0.7769\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.7373 - val_loss: 0.7765\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.7433 - val_loss: 0.7849\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7492 - val_loss: 0.7690\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.7314 - val_loss: 0.7666\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.7389 - val_loss: 0.7622\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7459 - val_loss: 0.7593\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.7433 - val_loss: 0.7672\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7438 - val_loss: 0.7638\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.7407 - val_loss: 0.7634\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.7431 - val_loss: 0.7629\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.7397 - val_loss: 0.7606\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7373 - val_loss: 0.7503\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.7305 - val_loss: 0.7494\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7148 - val_loss: 0.7210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.6958 - val_loss: 0.6904\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7084 - val_loss: 0.6871\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6977 - val_loss: 0.6877\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7045 - val_loss: 0.6894\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6965 - val_loss: 0.6912\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6993 - val_loss: 0.6894\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.6950 - val_loss: 0.6901\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6956 - val_loss: 0.6948\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6776 - val_loss: 0.6875\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6811 - val_loss: 0.6620\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6758 - val_loss: 0.6579\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 13740.8301 - val_loss: 1544235.3750\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 118610760.0000 - val_loss: 6191241.0000\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 7579902.5000 - val_loss: 0.6657\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 149ms/step - loss: 0.7180 - val_loss: 0.7010\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6949 - val_loss: 0.6772\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.7091 - val_loss: 0.6566\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 0.7104 - val_loss: 0.6654\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.6882 - val_loss: 0.6920\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6924 - val_loss: 0.7054\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6805 - val_loss: 0.7098\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6815 - val_loss: 0.6984\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6893 - val_loss: 0.6837\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6757 - val_loss: 0.6765\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6891 - val_loss: 0.6776\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6754 - val_loss: 0.6868\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 1411063680.0000 - val_loss: 0.7008\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.6802 - val_loss: 0.7030\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.6815 - val_loss: 0.7075\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.6836 - val_loss: 0.6903\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.6624 - val_loss: 0.6411\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.6675 - val_loss: 0.6268\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6598 - val_loss: 0.6307\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6464 - val_loss: 0.6500\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6524 - val_loss: 0.6532\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.6349 - val_loss: 0.6497\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.6501 - val_loss: 0.6452\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6537 - val_loss: 0.6339\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6519 - val_loss: 0.6330\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6511 - val_loss: 0.6475\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6541 - val_loss: 0.6497\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.6467 - val_loss: 0.6459\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6448 - val_loss: 0.6493\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6498 - val_loss: 0.6524\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7046 - val_loss: 0.7068\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.7169 - val_loss: 0.6697\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6999 - val_loss: 0.6718\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6941 - val_loss: 0.6839\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6846 - val_loss: 0.7051\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6801 - val_loss: 0.7026\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6908 - val_loss: 0.6971\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6934 - val_loss: 0.6964\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6829 - val_loss: 0.6994\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6839 - val_loss: 0.7005\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6823 - val_loss: 0.7070\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.6845 - val_loss: 0.7026\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6786 - val_loss: 0.7083\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.6930 - val_loss: 0.7074\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6768 - val_loss: 0.7044\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.7088 - val_loss: 0.7428\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.6936 - val_loss: 0.6792\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6879 - val_loss: 0.6661\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6784 - val_loss: 0.6656\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6745 - val_loss: 0.6619\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6854 - val_loss: 0.6595\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.6803 - val_loss: 0.6545\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6690 - val_loss: 0.6551\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6783 - val_loss: 0.6449\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6745 - val_loss: 0.6332\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 153.6806 - val_loss: 0.6624\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6750 - val_loss: 0.6782\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6723 - val_loss: 0.6862\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6729 - val_loss: 0.6897\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6756 - val_loss: 0.6912\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 0.7235 - val_loss: 0.7295\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.7230 - val_loss: 0.7059\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7157 - val_loss: 0.6892\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7089 - val_loss: 0.6951\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.7003 - val_loss: 0.7104\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6999 - val_loss: 0.7217\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7065 - val_loss: 0.7190\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7026 - val_loss: 0.7168\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7014 - val_loss: 0.7142\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.6998 - val_loss: 0.7025\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.7010 - val_loss: 0.6971\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6955 - val_loss: 0.7019\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6978 - val_loss: 0.7014\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6947 - val_loss: 0.7063\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6981 - val_loss: 0.7086\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.7272 - val_loss: 0.7325\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.7105 - val_loss: 0.6974\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7037 - val_loss: 0.6869\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6993 - val_loss: 0.6944\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6970 - val_loss: 0.6993\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7029 - val_loss: 0.7060\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.7031 - val_loss: 0.7067\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.7088 - val_loss: 0.7010\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.6963 - val_loss: 0.6917\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.6983 - val_loss: 0.6908\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.6954 - val_loss: 0.6950\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.7007 - val_loss: 0.7014\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6939 - val_loss: 0.7048\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6981 - val_loss: 0.7053\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.6962 - val_loss: 0.7029\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.7368 - val_loss: 0.7334\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7103 - val_loss: 0.6871\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6974 - val_loss: 0.6855\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.7103 - val_loss: 0.6971\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7085 - val_loss: 0.7120\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6871 - val_loss: 0.7241\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 729550.5625 - val_loss: 749262528.0000\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 316652448.0000 - val_loss: 132390848.0000\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 627727488.0000 - val_loss: 361416576.0000\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 1219729920.0000 - val_loss: 6125576.0000\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 1479783.6250 - val_loss: 45831.2930\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 394.9711 - val_loss: 0.7222\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6867 - val_loss: 0.7216\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6986 - val_loss: 0.7313\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.6990 - val_loss: 0.7311\n",
      "F1-Score: 95% confidence interval 0.000 and 0.460\n",
      "Matthews Corr Coef: 95% confidence interval -0.249 and 0.122\n"
     ]
    }
   ],
   "source": [
    "# Here is an attempt to learn from embeddings only\n",
    "\n",
    "def get_embedding_model():\n",
    "    inputs = tf.keras.layers.Input(shape=(MAX_LEN))\n",
    "    embedding = tf.keras.layers.Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    lstm = tf.keras.layers.LSTM(64, activation='relu', kernel_regularizer=L2(1e-6))(embedding)\n",
    "    dropout = tf.keras.layers.Dropout(0.4)(lstm)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0005))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_predict_embedding_model(x_train, x_test, y_train, y_test):\n",
    "    x_train_1, x_test_1 = get_embeddings_model_x_inputs(x_train, x_test)\n",
    "\n",
    "    with local_seed(10):\n",
    "        tf.keras.backend.clear_session()\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5)\n",
    "\n",
    "        model = get_embedding_model()\n",
    "        result = model.fit(x_train_1,\n",
    "            y_train, \n",
    "            batch_size = 128, \n",
    "            epochs=15,\n",
    "#             callbacks=[es],\n",
    "            validation_data=(x_test_1, y_test),\n",
    "            class_weight=class_weights, verbose=1)\n",
    "\n",
    "    y_pred = model.predict_step(x_test_1).numpy()\n",
    "    y_pred_bool = y_pred > 0.5\n",
    "    return evaluate_model(y_test, y_pred_bool)\n",
    "\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 10, test_size=0.2, shuffle=True)\n",
    "# train_and_predict_embedding_model(x_train, x_test, y_train, y_test)\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_embedding_model, 20, stratify=False, model_name=\"Fusion model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting in the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result :  {'f1': 0.732, 'mmc': 0.313685, 'acc': 0.672, 'precision': 0.698, 'recall': 0.769, 'tp': 30, 'fp': 13, 'tn': 15, 'fn': 9}\n",
      "[0.0837086  0.08476589 0.08434628 0.09016298 0.13661477 0.09820132\n",
      " 0.15285928 0.1438306  0.12551028]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X, Y = data_2_roles[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']].values, (data_2_roles['season_before_betrayal'].values <= 3).astype(np.int)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10000)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result :  {'f1': 0.622, 'mmc': 0.159145, 'acc': 0.582, 'precision': 0.657, 'recall': 0.59, 'tp': 23, 'fp': 12, 'tn': 16, 'fn': 16}\n",
      "[0.13518332 0.11615621 0.1170406  0.10638175 0.10464975 0.11983372\n",
      " 0.10538792 0.08388288 0.11148385]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "X, Y = data_2_roles[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']].values, (data_2_roles['season_before_betrayal'].values <= 3).astype(np.int)\n",
    "ratio = float(np.sum(Y == 0)) / np.sum(Y == 1) #for XGBoost\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = xgboost.XGBClassifier(n_jobs=-1, scale_pos_weight=ratio, n_estimators=300, max_depth=20)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Relative Importance')"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "features=['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']\n",
    "importances = clf.feature_importances_\n",
    "# indices = np.argsort(importances)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances, color='b', align='center')\n",
    "plt.yticks(range(len(indices)), features)\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result :  {'f1': 0.345, 'mmc': 0.129739, 'acc': 0.625, 'precision': 0.504, 'recall': 0.262, 'tp': 180, 'fp': 177, 'tn': 957, 'fn': 506}\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_2[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness']].values, (data_2['season_before_betrayal'].values <= 2).astype(np.int)\n",
    "\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.50, random_state=42)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, class_weight=class_weights)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result :  {'f1': 0.404, 'mmc': 0.033189, 'acc': 0.534, 'precision': 0.467, 'recall': 0.356, 'tp': 21, 'fp': 24, 'tn': 50, 'fn': 38}\n",
      "[0.12506528 0.11655457 0.11377538 0.13706627 0.10561607 0.09841212\n",
      " 0.09687702 0.09557363 0.11105967]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "X, Y = data_2_roles[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']].values, (data_2_roles['season_before_betrayal'].values <= 2).astype(np.int)\n",
    "ratio = float(np.sum(Y == 0)) / np.sum(Y == 1) #for XGBoost\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = xgboost.XGBClassifier(n_jobs=-1, scale_pos_weight=ratio, n_estimators=300, max_depth=20)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foretell much from the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2_roles_seaons_2 = data_2_roles[data_2_roles['season_before_betrayal'] >= 2]\n",
    "data_2_roles_seaons_3 = data_2_roles[data_2_roles['season_before_betrayal'] >= 3]\n",
    "data_2_roles_seaons_4 = data_2_roles[data_2_roles['season_before_betrayal'] >= 4]\n",
    "data_2_roles_seaons_4 = data_2_roles[data_2_roles['season_before_betrayal'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>role</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>politeness</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "      <th>delta_role</th>\n",
       "      <th>delta_time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1906.5</th>\n",
       "      <td>betrayer</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>81.5</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>6.0</td>\n",
       "      <td>-0.006665</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907.0</th>\n",
       "      <td>betrayer</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>40.0</td>\n",
       "      <td>2.285714</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.225425</td>\n",
       "      <td>-0.243245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1907.5</th>\n",
       "      <td>betrayer</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>166.5</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.375372</td>\n",
       "      <td>0.422620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908.0</th>\n",
       "      <td>betrayer</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>89.8</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.177667</td>\n",
       "      <td>-0.233901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1908.5</th>\n",
       "      <td>betrayer</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.899161</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.335738</td>\n",
       "      <td>0.150360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                role  sentiment_positive  sentiment_neutral  \\\n",
       "idx season                                                    \n",
       "0   1906.5  betrayer            1.333333           1.333333   \n",
       "    1907.0  betrayer            0.142857           0.857143   \n",
       "    1907.5  betrayer            2.000000           2.500000   \n",
       "    1908.0  betrayer            1.800000           0.800000   \n",
       "    1908.5  betrayer            1.000000           1.000000   \n",
       "\n",
       "            sentiment_negative  n_requests  n_words  n_sentences  politeness  \\\n",
       "idx season                                                                     \n",
       "0   1906.5            1.500000    3.666667     81.5     4.166667    0.803328   \n",
       "    1907.0            1.285714    1.285714     40.0     2.285714    0.560083   \n",
       "    1907.5            2.000000    5.500000    166.5     6.500000    0.982703   \n",
       "    1908.0            2.200000    3.200000     89.8     4.800000    0.748802   \n",
       "    1908.5            1.000000    2.000000     39.0     3.000000    0.899161   \n",
       "\n",
       "            season_before_betrayal  delta_role  delta_time  \n",
       "idx season                                                  \n",
       "0   1906.5                     6.0   -0.006665    0.000000  \n",
       "    1907.0                     5.0   -0.225425   -0.243245  \n",
       "    1907.5                     4.0    0.375372    0.422620  \n",
       "    1908.0                     3.0    0.177667   -0.233901  \n",
       "    1908.5                     2.0    0.335738    0.150360  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2_roles_seaons_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6846153846153846, 1: 1.8541666666666667}\n",
      "result :  {'f1': 0.189, 'mmc': 0.20226, 'acc': 0.733, 'precision': 0.714, 'recall': 0.109, 'tp': 5, 'fp': 2, 'tn': 113, 'fn': 41}\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_2_roles_seaons_2[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']].values, (data_2_roles_seaons_2['season_before_betrayal'].values == 2).astype(np.int)\n",
    "\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "class_weights = {0: weights[0], 1: weights[1a\n",
    "\n",
    "print(class_weights)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42, shuffle=True)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,) # class_weight=class_weights)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.6890459363957597, 1: 1.8224299065420562}\n",
      "result :  {'f1': 0.267, 'mmc': 0.136685, 'acc': 0.718, 'precision': 0.429, 'recall': 0.194, 'tp': 6, 'fp': 8, 'tn': 78, 'fn': 25}\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_2_roles_seaons_3[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']].values, (data_2_roles_seaons_3['season_before_betrayal'].values == 3).astype(np.int)\n",
    "\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "print(class_weights)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42, shuffle=True)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,) # class_weight=class_weights)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.7408376963350786, 1: 1.5380434782608696}\n",
      "result :  {'f1': 0.25, 'mmc': 0.165342, 'acc': 0.647, 'precision': 0.625, 'recall': 0.156, 'tp': 5, 'fp': 3, 'tn': 50, 'fn': 27}\n"
     ]
    }
   ],
   "source": [
    "X, Y = data_2_roles_seaons_4[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']].values, (data_2_roles_seaons_4['season_before_betrayal'].values == 4).astype(np.int)\n",
    "\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "print(class_weights)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42, shuffle=True)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,) # class_weight=class_weights)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_2_roles_seaons_5[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']].values, (data_2_roles_seaons_5['season_before_betrayal'].values == 5).astype(np.int)\n",
    "\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "\n",
    "print(class_weights)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, random_state=42, shuffle=True)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100,) # class_weight=class_weights)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result :  {'f1': 0.211, 'mmc': 0.025482, 'acc': 0.605, 'precision': 0.4, 'recall': 0.143, 'tp': 4, 'fp': 6, 'tn': 42, 'fn': 24}\n",
      "[0.09449852 0.1036016  0.12157353 0.13993303 0.09435733 0.13205571\n",
      " 0.10808396 0.10805522 0.09784104]\n"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "X, Y = data_2_roles_seaons_3[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness', 'delta_role', 'delta_time']].values, (data_2_roles_seaons_3['season_before_betrayal'].values == 3).astype(np.int)\n",
    "ratio = float(np.sum(Y == 0)) / np.sum(Y == 1) #for XGBoost\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "clf = xgboost.XGBClassifier(n_jobs=-1, scale_pos_weight=ratio, n_estimators=300, max_depth=20)\n",
    "clf.fit(x_train_nor, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test_nor)\n",
    "\n",
    "print('result : ', evaluate_model(y_test, y_pred))\n",
    "print(clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_search' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-5b2d6daedf6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best parameters: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best cross-validation f1 score: {:.2f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_search' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation f1 score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
