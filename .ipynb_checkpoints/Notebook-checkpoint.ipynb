{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import spacy\n",
    "#import gensim.downloader as api\n",
    "\n",
    "#info = api.info()  # show info about available models/datasets\n",
    "#model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "\n",
    "#nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Readme here:\n",
    "https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use here the dataset provided by the authors we will create a helper function to extract the features that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seasons</th>\n",
       "      <th>game</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>idx</th>\n",
       "      <th>people</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'season': 1906.5, 'interaction': {'victim': ...</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>AT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'season': 1911.5, 'interaction': {'victim': ...</td>\n",
       "      <td>165</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             seasons  game  betrayal  idx  \\\n",
       "0  [{'season': 1906.5, 'interaction': {'victim': ...    74      True    0   \n",
       "1  [{'season': 1911.5, 'interaction': {'victim': ...   165     False    1   \n",
       "\n",
       "  people  \n",
       "0     AT  \n",
       "1     EG  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"diplomacy_data/diplomacy_data.json\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_support(entry):\n",
    "    \"\"\"\n",
    "    This function returns the last season of friendship. The code is inspired by the provided code from\n",
    "    the authors\n",
    "    \"\"\"\n",
    "    last_support = None\n",
    "    for season in entry[:-1]:\n",
    "        if 'support' in season['interaction'].values():\n",
    "            last_support = season['season']\n",
    "    return last_support\n",
    "\n",
    "def treat_msg_season(df):\n",
    "    \"\"\"\n",
    "    This function loops over the whole dataset and creates a dictionnary with the set of features for each season \n",
    "    with its associated boolean (betrayal or not )\n",
    "    \"\"\"\n",
    "    data_victim = {'features':[], 'betrayed':[]} # data of the (potential) victim \n",
    "    data_betrayer = {'features':[], 'betrayed':[]} # data of the (potential) betrayer\n",
    "    for i in range(len(df.seasons.values)):\n",
    "        entry = df['seasons'][i] # pick each entry\n",
    "        for j in range(len(entry)): # pick each season\n",
    "            season = entry[j]\n",
    "            tab_vi = []\n",
    "            tab_be = []\n",
    "            if season['season'] <= last_support(entry): # check if the season is below the last season of friendship\n",
    "                tab_vi.append(season['messages']['victim'])\n",
    "                tab_be.append(season['messages']['betrayer'])\n",
    "                if len(tab_be) != 0 and len(tab_vi) != 0: # keep only cases where both players have sent messages\n",
    "                    data_victim['features'].append(tab_vi)\n",
    "                    data_victim['betrayed'].append(df.betrayal.values[i])\n",
    "                    data_betrayer['features'].append(tab_be)   \n",
    "                    data_betrayer['betrayed'].append(df.betrayal.values[i])\n",
    "    return data_victim, data_betrayer\n",
    "\n",
    "data_victim, data_betrayer = treat_msg_season(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(message):\n",
    "    sentiment_positive = message['sentiment']['positive']\n",
    "    sentiment_neutral = message['sentiment']['neutral']\n",
    "    sentiment_negative = message['sentiment']['negative']\n",
    "    n_requests = message['n_requests']\n",
    "    frequent_words = message['frequent_words']\n",
    "    n_words = message['n_words']\n",
    "    politeness = message['politeness']\n",
    "    n_sentences = message['n_sentences']\n",
    "    return {\"sentiment_positive\": sentiment_positive,\n",
    "           \"sentiment_neutral\": sentiment_neutral,\n",
    "           'sentiment_negative': sentiment_negative,\n",
    "           'n_requests': n_requests,\n",
    "           'frequent_words': frequent_words,\n",
    "           'n_words': n_words,\n",
    "           'politeness': politeness,\n",
    "           'n_sentences': n_sentences}\n",
    "    \n",
    "\n",
    "\n",
    "def preprocessing(df):\n",
    "    result = []\n",
    "    for row in df.iterrows():\n",
    "        row = row[1]\n",
    "        betrayal = row['betrayal']\n",
    "        idx = row['idx']\n",
    "        for season in row['seasons']:\n",
    "            s = season['season']\n",
    "                \n",
    "            last_s = last_support(row['seasons'])+0.5 # the betrayal occurs one season after the last support\n",
    "            if s <= last_support(row['seasons']) and len(season['messages']['betrayer']) and len(season['messages']['victim']): # here we also have to consider the last season before betrayal\n",
    "                interaction_victim = season['interaction']['victim']\n",
    "                interaction_betrayer = season ['interaction']['betrayer']\n",
    "                for m_vic in season['messages']['victim']:\n",
    "                    data = to_dict(m_vic)\n",
    "                    data['role'] = 'victim'\n",
    "                    data['season'] = s\n",
    "                    data['betrayal'] = betrayal\n",
    "                    data['season_betrayal'] = last_s\n",
    "                    data['season_before_betrayal'] = (last_s-s)/0.5\n",
    "                    data['idx'] = idx\n",
    "                    result.append(data)\n",
    "                for m_bet in season['messages']['betrayer']:\n",
    "                    data = to_dict(m_bet)\n",
    "                    data['role'] = 'betrayer'\n",
    "                    data['season'] = s\n",
    "                    data['betrayal'] = betrayal\n",
    "                    data['season_betrayal'] = last_s\n",
    "                    data['season_before_betrayal'] = (last_s-s)/0.5\n",
    "                    data['idx'] = idx\n",
    "                    result.append(data)\n",
    "                            \n",
    "    return pd.DataFrame(result).set_index(['idx', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1906.5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, bot, ,, ., take, unit, war, retreat, di...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>2</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[armies, north, the, armies, on, ., your, with...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.932326</td>\n",
       "      <td>6</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[?, going, for, ser, balance, a, to, of, give,...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>4</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>[only, he, alb, ., forced, italy's, is, be, .,...</td>\n",
       "      <td>313</td>\n",
       "      <td>0.957072</td>\n",
       "      <td>19</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>[more, let, keep, we, side, we, don't, to, ., ...</td>\n",
       "      <td>146</td>\n",
       "      <td>0.832023</td>\n",
       "      <td>9</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "idx season                                                              \n",
       "0   1906.5                   0                  0                   2   \n",
       "    1906.5                   1                  1                   4   \n",
       "    1906.5                   1                  2                   1   \n",
       "    1906.5                   4                  2                  13   \n",
       "    1906.5                   1                  3                   5   \n",
       "\n",
       "            n_requests                                     frequent_words  \\\n",
       "idx season                                                                  \n",
       "0   1906.5           1  [just, bot, ,, ., take, unit, war, retreat, di...   \n",
       "    1906.5           2  [armies, north, the, armies, on, ., your, with...   \n",
       "    1906.5           2  [?, going, for, ser, balance, a, to, of, give,...   \n",
       "    1906.5           8  [only, he, alb, ., forced, italy's, is, be, .,...   \n",
       "    1906.5           7  [more, let, keep, we, side, we, don't, to, ., ...   \n",
       "\n",
       "            n_words  politeness  n_sentences      role  betrayal  \\\n",
       "idx season                                                         \n",
       "0   1906.5       35    0.367200            2    victim      True   \n",
       "    1906.5       77    0.932326            6    victim      True   \n",
       "    1906.5       55    0.983373            4    victim      True   \n",
       "    1906.5      313    0.957072           19    victim      True   \n",
       "    1906.5      146    0.832023            9  betrayer      True   \n",
       "\n",
       "            season_betrayal  season_before_betrayal  \n",
       "idx season                                           \n",
       "0   1906.5           1909.5                     6.0  \n",
       "    1906.5           1909.5                     6.0  \n",
       "    1906.5           1909.5                     6.0  \n",
       "    1906.5           1909.5                     6.0  \n",
       "    1906.5           1909.5                     6.0  "
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessing(data)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In each season, potential betrayers send in average 1.627498001598721, with a maximum of 38 messages\n",
      "In each season, potential victims send in average 1.515587529976019, with a maximum of 28 messages\n"
     ]
    }
   ],
   "source": [
    "def get_nb_msg(data):\n",
    "    \"\"\"\n",
    "    Get the mean number of messages sent per season\n",
    "    \"\"\"\n",
    "    tab = []\n",
    "    for features in data[\"features\"]:\n",
    "        tab.append(len(features[0]))\n",
    "    return tab\n",
    "\n",
    "print(\"In each season, potential betrayers send in average {}, with a maximum of {} messages\".format(np.mean(get_nb_msg(data_betrayer)), np.max(get_nb_msg(data_betrayer))))\n",
    "print(\"In each season, potential victims send in average {}, with a maximum of {} messages\".format(np.mean(get_nb_msg(data_victim)), np.max(get_nb_msg(data_victim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_words(entry):\n",
    "    \"\"\"\n",
    "    get the set of lexicon words for each entry of the dataset\n",
    "    1 entry = 1 row = 1 set of messages\n",
    "    Can be improved\n",
    "    \"\"\"\n",
    "    for entries in entry[0]: #loop over the messages\n",
    "        # get the lexicon words\n",
    "        di_words = entries[\"lexicon_words\"]\n",
    "        tab_words = []\n",
    "        for key in di_words:\n",
    "            tab = di_words[key]\n",
    "            for words in tab:\n",
    "                word = words.split(' ')\n",
    "                for w in word:\n",
    "                    if w not in tab_words:\n",
    "                        tab_words.append(w)\n",
    "    return tab_words\n",
    "\n",
    "test = get_lexicon_words(data_victim[\"features\"][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season_before_betrayal\n",
       "1.0    343\n",
       "2.0    360\n",
       "3.0    314\n",
       "4.0    277\n",
       "5.0    231\n",
       "6.0    156\n",
       "7.0     94\n",
       "8.0     78\n",
       "9.0     42\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"betrayal\"]==True) & (df[\"role\"] == \"betrayer\")].reset_index().groupby('season_before_betrayal').count()['idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 367 instances of 1 season before betrayal, 379 instances of 2 seasons before betrayal etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting iminent betrayal - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider only messages that have been exchanged one season before the betrayal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[would, on, other, france, i, that, to, think,...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.951652</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, the, the, ,, game, that, unless, on, wa...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.867535</td>\n",
       "      <td>2</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[france, for, see, now, would, did, just, what...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.464116</td>\n",
       "      <td>1</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[bla, not, of, always, a, if, goes, take, ,, t...</td>\n",
       "      <td>134</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>6</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[about, to, aeg, was, now, anyway, has, gre, b...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.218056</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[war, after, and, which, that, into, may, you,...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.784303</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[another, game, ., we, i, ., think, 6, have, ....</td>\n",
       "      <td>18</td>\n",
       "      <td>0.690164</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[vie, into, and, ., you, tun, also, ahead, sup...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.600676</td>\n",
       "      <td>2</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[and, through, ., sounds, enter, to, i'm, move...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.852749</td>\n",
       "      <td>4</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[on, talk, ., write, ., and, if, more, perfect...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  season  sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "0    0  1909.0                   0                  1                   2   \n",
       "1    0  1909.0                   0                  0                   2   \n",
       "2    0  1909.0                   0                  1                   0   \n",
       "3    3  1902.5                   0                  2                   4   \n",
       "4    3  1902.5                   1                  0                   2   \n",
       "5    4  1911.0                   0                  2                   1   \n",
       "6    4  1911.0                   0                  3                   0   \n",
       "7    4  1911.0                   1                  1                   0   \n",
       "8    5  1906.0                   3                  0                   1   \n",
       "9    8  1906.0                   1                  2                   0   \n",
       "\n",
       "   n_requests                                     frequent_words  n_words  \\\n",
       "0           2  [would, on, other, france, i, that, to, think,...       42   \n",
       "1           1  [know, the, the, ,, game, that, unless, on, wa...       51   \n",
       "2           1  [france, for, see, now, would, did, just, what...       14   \n",
       "3           4  [bla, not, of, always, a, if, goes, take, ,, t...      134   \n",
       "4           2  [about, to, aeg, was, now, anyway, has, gre, b...       62   \n",
       "5           2  [war, after, and, which, that, into, may, you,...       44   \n",
       "6           0  [another, game, ., we, i, ., think, 6, have, ....       18   \n",
       "7           0  [vie, into, and, ., you, tun, also, ahead, sup...       17   \n",
       "8           1  [and, through, ., sounds, enter, to, i'm, move...       33   \n",
       "9           1  [on, talk, ., write, ., and, if, more, perfect...       20   \n",
       "\n",
       "   politeness  n_sentences      role  betrayal  season_betrayal  \\\n",
       "0    0.951652            3  betrayer      True           1909.5   \n",
       "1    0.867535            2  betrayer      True           1909.5   \n",
       "2    0.464116            1  betrayer      True           1909.5   \n",
       "3    0.855036            6  betrayer     False           1903.0   \n",
       "4    0.218056            3  betrayer     False           1903.0   \n",
       "5    0.784303            3  betrayer     False           1911.5   \n",
       "6    0.690164            3  betrayer     False           1911.5   \n",
       "7    0.600676            2  betrayer     False           1911.5   \n",
       "8    0.852749            4  betrayer      True           1906.5   \n",
       "9    0.480527            3  betrayer      True           1906.5   \n",
       "\n",
       "   season_before_betrayal  \n",
       "0                     1.0  \n",
       "1                     1.0  \n",
       "2                     1.0  \n",
       "3                     1.0  \n",
       "4                     1.0  \n",
       "5                     1.0  \n",
       "6                     1.0  \n",
       "7                     1.0  \n",
       "8                     1.0  \n",
       "9                     1.0  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = df[(df[\"season_before_betrayal\"]<=1) & (df[\"role\"] == \"betrayer\")].reset_index().copy()\n",
    "data_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[would, on, other, france, i, that, to, think,...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.951652</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, the, the, ,, game, that, unless, on, wa...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.867535</td>\n",
       "      <td>2</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[france, for, see, now, would, did, just, what...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.464116</td>\n",
       "      <td>1</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[bla, not, of, always, a, if, goes, take, ,, t...</td>\n",
       "      <td>134</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>6</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[about, to, aeg, was, now, anyway, has, gre, b...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.218056</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[war, after, and, which, that, into, may, you,...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.784303</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[another, game, ., we, i, ., think, 6, have, ....</td>\n",
       "      <td>18</td>\n",
       "      <td>0.690164</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[vie, into, and, ., you, tun, also, ahead, sup...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.600676</td>\n",
       "      <td>2</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[and, through, ., sounds, enter, to, i'm, move...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.852749</td>\n",
       "      <td>4</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[on, talk, ., write, ., and, if, more, perfect...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  season  sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "0    0  1909.0                   0                  1                   2   \n",
       "1    0  1909.0                   0                  0                   2   \n",
       "2    0  1909.0                   0                  1                   0   \n",
       "3    3  1902.5                   0                  2                   4   \n",
       "4    3  1902.5                   1                  0                   2   \n",
       "5    4  1911.0                   0                  2                   1   \n",
       "6    4  1911.0                   0                  3                   0   \n",
       "7    4  1911.0                   1                  1                   0   \n",
       "8    5  1906.0                   3                  0                   1   \n",
       "9    8  1906.0                   1                  2                   0   \n",
       "\n",
       "   n_requests                                     frequent_words  n_words  \\\n",
       "0           2  [would, on, other, france, i, that, to, think,...       42   \n",
       "1           1  [know, the, the, ,, game, that, unless, on, wa...       51   \n",
       "2           1  [france, for, see, now, would, did, just, what...       14   \n",
       "3           4  [bla, not, of, always, a, if, goes, take, ,, t...      134   \n",
       "4           2  [about, to, aeg, was, now, anyway, has, gre, b...       62   \n",
       "5           2  [war, after, and, which, that, into, may, you,...       44   \n",
       "6           0  [another, game, ., we, i, ., think, 6, have, ....       18   \n",
       "7           0  [vie, into, and, ., you, tun, also, ahead, sup...       17   \n",
       "8           1  [and, through, ., sounds, enter, to, i'm, move...       33   \n",
       "9           1  [on, talk, ., write, ., and, if, more, perfect...       20   \n",
       "\n",
       "   politeness  n_sentences      role  betrayal  season_betrayal  \\\n",
       "0    0.951652            3  betrayer      True           1909.5   \n",
       "1    0.867535            2  betrayer      True           1909.5   \n",
       "2    0.464116            1  betrayer      True           1909.5   \n",
       "3    0.855036            6  betrayer     False           1903.0   \n",
       "4    0.218056            3  betrayer     False           1903.0   \n",
       "5    0.784303            3  betrayer     False           1911.5   \n",
       "6    0.690164            3  betrayer     False           1911.5   \n",
       "7    0.600676            2  betrayer     False           1911.5   \n",
       "8    0.852749            4  betrayer      True           1906.5   \n",
       "9    0.480527            3  betrayer      True           1906.5   \n",
       "\n",
       "   season_before_betrayal  \n",
       "0                     1.0  \n",
       "1                     1.0  \n",
       "2                     1.0  \n",
       "3                     1.0  \n",
       "4                     1.0  \n",
       "5                     1.0  \n",
       "6                     1.0  \n",
       "7                     1.0  \n",
       "8                     1.0  \n",
       "9                     1.0  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_1[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', 'n_sentences', 'politeness']].values, 1*data_1['betrayal'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "def normalize(x):\n",
    "    x_temp = x.copy()\n",
    "    for i in range(6):\n",
    "        m = np.mean(x[:, i])\n",
    "        s = np.std(x[:, i])\n",
    "        x_temp[:, i] -= m\n",
    "        x_temp[:, i] /= s\n",
    "    return x_temp\n",
    "\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42424242424242425"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(x_train_nor, y_train)\n",
    "clf.score(x_test_nor, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48484848484848486"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
    "clf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The feed forward model + some preprocessing to understand better the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = df.copy()\n",
    "\n",
    "# Now train just for the betrayer role for changes, although we could add data for both?\n",
    "features_data = features_data[features_data['role'] == 'betrayer']\n",
    "features_data = features_data[features_data['betrayal'] == True]\n",
    "\n",
    "features_data = features_data.drop(columns=['frequent_words', 'season_betrayal', 'role', 'betrayal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggreagted_features_per_season = features_data.groupby(['idx', 'season'], as_index=False).aggregate({\n",
    "    'sentiment_positive': 'mean',\n",
    "    'sentiment_neutral': 'mean',\n",
    "    'sentiment_negative': 'mean',\n",
    "    'n_requests': 'mean',\n",
    "    'n_words': 'sum',\n",
    "    'politeness': 'mean',\n",
    "    'n_sentences': 'sum',\n",
    "    'season_before_betrayal': 'min'\n",
    "})\n",
    "\n",
    "X = aggreagted_features_per_season[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',\n",
    "       'n_requests', 'n_words', 'politeness', 'n_sentences']]\n",
    "Y = (aggreagted_features_per_season['season_before_betrayal'] == 1.0).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>489</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>280</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>333</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>449</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>0.899161</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_positive  sentiment_neutral  sentiment_negative  n_requests  \\\n",
       "0            1.333333           1.333333            1.500000    3.666667   \n",
       "1            0.142857           0.857143            1.285714    1.285714   \n",
       "2            2.000000           2.500000            2.000000    5.500000   \n",
       "3            1.800000           0.800000            2.200000    3.200000   \n",
       "4            1.000000           1.000000            1.000000    2.000000   \n",
       "\n",
       "   n_words  politeness  n_sentences  \n",
       "0      489    0.803328           25  \n",
       "1      280    0.560083           16  \n",
       "2      333    0.982703           13  \n",
       "3      449    0.748802           24  \n",
       "4       78    0.899161            6  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(663, 7)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19457013574660634"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check imbalance of labels\n",
    "np.sum(Y == 1)/(Y.shape[0]) # ~18%; idk why not ~14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, shuffle=True, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train) # use the mean, std of the training to standardize the test\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1, L2, L1L2\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "# Please note that these are computed per batch, not on entire validation\n",
    "# and might be misleading\n",
    "METRICS = [\n",
    "      metrics.TruePositives(name='tp'),\n",
    "      metrics.FalsePositives(name='fp'),\n",
    "      metrics.TrueNegatives(name='tn'),\n",
    "      metrics.FalseNegatives(name='fn'),\n",
    "      metrics.BinaryAccuracy(name='accuracy'),\n",
    "      metrics.Precision(name='precision'),\n",
    "      metrics.Recall(name='recall'),\n",
    "      metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "FEATURES_NUM = X.shape[-1]\n",
    "\n",
    "def get_feed_forward_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=512, activation='relu', kernel_regularizer=L1L2(1e-6), input_shape=(FEATURES_NUM,)))\n",
    "    model.add(Dense(units=256, activation='relu', kernel_regularizer=L1L2(1e-6)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.00001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=METRICS)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.0410094637223974, 1: 0.9620991253644315}"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To address imbalance, compute the weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_152\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_460 (Dense)            (None, 512)               4096      \n",
      "_________________________________________________________________\n",
      "dense_461 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_152 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_462 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 135,681\n",
      "Trainable params: 135,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 00021: early stopping\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model_tmp.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "model = get_feed_forward_model()\n",
    "result = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=(x_test, y_test), \n",
    "    callbacks = [es, mc],\n",
    "    class_weight=class_weights, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(result.history['loss'], label='train')\n",
    "pyplot.plot(result.history['val_loss'], label='test')\n",
    "\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 594us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 33.636,\n",
       " 'f1': 0.705,\n",
       " 'fn': 408,\n",
       " 'fp': 1728,\n",
       " 'mmc': 0.250102,\n",
       " 'precision': 0.515,\n",
       " 'recall': 0.818,\n",
       " 'tn': 384,\n",
       " 'tp': 1836}"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# TODO: apply bootstrapping \n",
    "def matthews_corr_coef(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TP, FP, FN, TN = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = np.sum(y_pred == y_true) / n\n",
    "    \n",
    "    mmc = matthews_corr_coef(y_true, y_pred)\n",
    "    \n",
    "    return {'f1': np.round(f1, decimals=3),\n",
    "            'mmc': np.round(mmc, decimals=6),\n",
    "            'acc': np.round(accuracy, decimals=3),\n",
    "            'precision': np.round(precision, decimals=3),\n",
    "            'recall': np.round(recall, decimals=3),\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'tn': tn,\n",
    "            'fn': fn\n",
    "           }\n",
    "\n",
    "# This results are cool, but they are much lower when we will do bootstrapping\n",
    "model.load_weights('best_model.h5')\n",
    "y_pred = model.predict(x_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "y_pred_bool = y_pred > 0.5\n",
    "evaluate_model(y_test, y_pred_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short attempt to the time series idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 3\n",
    "\n",
    "def split_sequence(X_data, y_data, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(X_data)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(X_data)-1:\n",
    "            break\n",
    "        seq_x = X_data[i:end_ix]\n",
    "        seq_y = y_data[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xp = scaler.fit_transform(X)\n",
    "x_seq, y_seq = split_sequence(Xp, Y, TIMESTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_seq, y_seq, test_size=0.10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "19/19 [==============================] - 1s 50ms/step - loss: 0.7019 - tp: 220.0000 - fp: 202.0000 - tn: 115.0000 - fn: 120.0000 - accuracy: 0.5099 - precision: 0.5213 - recall: 0.6471 - auc: 0.5125 - val_loss: 0.6713 - val_tp: 26.0000 - val_fp: 20.0000 - val_tn: 15.0000 - val_fn: 5.0000 - val_accuracy: 0.6212 - val_precision: 0.5652 - val_recall: 0.8387 - val_auc: 0.7120\n",
      "Epoch 2/100\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.6997 - tp: 216.0000 - fp: 182.0000 - tn: 100.0000 - fn: 93.0000 - accuracy: 0.5347 - precision: 0.5427 - recall: 0.6990 - auc: 0.5275 - val_loss: 0.6716 - val_tp: 26.0000 - val_fp: 20.0000 - val_tn: 15.0000 - val_fn: 5.0000 - val_accuracy: 0.6212 - val_precision: 0.5652 - val_recall: 0.8387 - val_auc: 0.7152\n",
      "Epoch 3/100\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.6961 - tp: 202.0000 - fp: 172.0000 - tn: 110.0000 - fn: 107.0000 - accuracy: 0.5279 - precision: 0.5401 - recall: 0.6537 - auc: 0.5279 - val_loss: 0.6720 - val_tp: 26.0000 - val_fp: 20.0000 - val_tn: 15.0000 - val_fn: 5.0000 - val_accuracy: 0.6212 - val_precision: 0.5652 - val_recall: 0.8387 - val_auc: 0.7147\n",
      "Epoch 4/100\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.7033 - tp: 188.0000 - fp: 185.0000 - tn: 97.0000 - fn: 121.0000 - accuracy: 0.4822 - precision: 0.5040 - recall: 0.6084 - auc: 0.4897 - val_loss: 0.6725 - val_tp: 26.0000 - val_fp: 20.0000 - val_tn: 15.0000 - val_fn: 5.0000 - val_accuracy: 0.6212 - val_precision: 0.5652 - val_recall: 0.8387 - val_auc: 0.7111\n",
      "Epoch 00004: early stopping\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7ff134f88200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 32.212,\n",
       " 'f1': 0.675,\n",
       " 'fn': 620,\n",
       " 'fp': 1610,\n",
       " 'mmc': 0.290261,\n",
       " 'precision': 0.47,\n",
       " 'recall': 0.697,\n",
       " 'tn': 700,\n",
       " 'tp': 1426}"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vanilla_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.LSTM(30, activation='relu', kernel_regularizer=keras.regularizers.L1L2(1e-6), input_shape=(TIMESTEPS, FEATURES_NUM)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "    return model\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model_tmp.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "model = get_vanilla_lstm_model()\n",
    "result = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=(x_test, y_test), \n",
    "    callbacks = [es, mc],\n",
    "    class_weight=class_weights, verbose=1)\n",
    "\n",
    "model.load_weights('best_model_tmp.h5')\n",
    "y_pred = model.predict(x_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "y_pred_bool = y_pred > 0.5\n",
    "evaluate_model(y_test, y_pred_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codecs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fdac5f6430fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"helpers/stopwords.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'codecs' is not defined"
     ]
    }
   ],
   "source": [
    "with codecs.open(\"helpers/stopwords.txt\", encoding='utf-8') as h:\n",
    "    stopwords = h.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy & Glove word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the spacy library to compute the embeddings. We have to try also with Glove and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    return nlp(word).vector\n",
    "\n",
    "def get_word_embedding_model(model, word):\n",
    "    return model[word]\n",
    "\n",
    "print(\"Word :{} , embedding : {}\".format(test[1], get_word_embedding_model(model, test[1])))\n",
    "#model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
