{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "import contextlib\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import L1, L2, L1L2\n",
    "from tensorflow.keras import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use here the dataset provided by the authors we will create a helper function to extract the features that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seasons</th>\n",
       "      <th>game</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>idx</th>\n",
       "      <th>people</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'season': 1906.5, 'interaction': {'victim': ...</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>AT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'season': 1911.5, 'interaction': {'victim': ...</td>\n",
       "      <td>165</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             seasons  game  betrayal  idx  \\\n",
       "0  [{'season': 1906.5, 'interaction': {'victim': ...    74      True    0   \n",
       "1  [{'season': 1911.5, 'interaction': {'victim': ...   165     False    1   \n",
       "\n",
       "  people  \n",
       "0     AT  \n",
       "1     EG  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"diplomacy_data/diplomacy_data.json\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_support(entry):\n",
    "    \"\"\"\n",
    "    This function returns the last season of friendship. The code is inspired by the provided code from\n",
    "    the authors\n",
    "    \"\"\"\n",
    "    last_support = None\n",
    "    for season in entry[:-1]:\n",
    "        if 'support' in season['interaction'].values():\n",
    "            last_support = season['season']\n",
    "    return last_support\n",
    "\n",
    "def treat_msg_season(df):\n",
    "    \"\"\"\n",
    "    This function loops over the whole dataset and creates a dictionnary with the set of features for each season \n",
    "    with its associated boolean (betrayal or not )\n",
    "    \"\"\"\n",
    "    data_victim = {'features':[], 'betrayed':[]} # data of the (potential) victim \n",
    "    data_betrayer = {'features':[], 'betrayed':[]} # data of the (potential) betrayer\n",
    "    for i in range(len(df.seasons.values)):\n",
    "        entry = df['seasons'][i] # pick each entry\n",
    "        for j in range(len(entry)): # pick each season\n",
    "            season = entry[j]\n",
    "            tab_vi = []\n",
    "            tab_be = []\n",
    "            if season['season'] <= last_support(entry): # check if the season is below the last season of friendship\n",
    "                tab_vi.append(season['messages']['victim'])\n",
    "                tab_be.append(season['messages']['betrayer'])\n",
    "                if len(tab_be) != 0 and len(tab_vi) != 0: # keep only cases where both players have sent messages\n",
    "                    data_victim['features'].append(tab_vi)\n",
    "                    data_victim['betrayed'].append(df.betrayal.values[i])\n",
    "                    data_betrayer['features'].append(tab_be)   \n",
    "                    data_betrayer['betrayed'].append(df.betrayal.values[i])\n",
    "    return data_victim, data_betrayer\n",
    "\n",
    "data_victim, data_betrayer = treat_msg_season(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_unique_words(message):\n",
    "    words = message['frequent_words']\n",
    "    for _, value in message['lexicon_words'].items():\n",
    "        words += value\n",
    "    \n",
    "    return list(set(words))\n",
    "        \n",
    "    \n",
    "def to_dict(message):\n",
    "    sentiment_positive = message['sentiment']['positive']\n",
    "    sentiment_neutral = message['sentiment']['neutral']\n",
    "    sentiment_negative = message['sentiment']['negative']\n",
    "    n_requests = message['n_requests']\n",
    "    frequent_words = message['frequent_words']\n",
    "    all_words = collect_all_unique_words(message)\n",
    "    n_words = message['n_words']\n",
    "    politeness = message['politeness']\n",
    "    n_sentences = message['n_sentences']\n",
    "    return {\"sentiment_positive\": sentiment_positive,\n",
    "           \"sentiment_neutral\": sentiment_neutral,\n",
    "           'sentiment_negative': sentiment_negative,\n",
    "           'n_requests': n_requests,\n",
    "           'frequent_words': frequent_words,\n",
    "           'n_words': n_words,\n",
    "           'politeness': politeness,\n",
    "           'n_sentences': n_sentences,\n",
    "           'all_words': all_words}\n",
    "\n",
    "\n",
    "def preprocessing(df):\n",
    "    result = []\n",
    "    for row in df.iterrows():\n",
    "        row = row[1]\n",
    "        betrayal = row['betrayal']\n",
    "        idx = row['idx']\n",
    "        for season in row['seasons']:\n",
    "            s = season['season']\n",
    "                \n",
    "            last_s = last_support(row['seasons'])+0.5 # the betrayal occurs one season after the last support\n",
    "            if s <= last_support(row['seasons']) and len(season['messages']['betrayer']) and len(season['messages']['victim']): # here we also have to consider the last season before betrayal\n",
    "                interaction_victim = season['interaction']['victim']\n",
    "                interaction_betrayer = season ['interaction']['betrayer']\n",
    "                for m_vic in season['messages']['victim']:\n",
    "                    data = to_dict(m_vic)\n",
    "                    data['role'] = 'victim'\n",
    "                    data['season'] = s\n",
    "                    data['betrayal'] = betrayal\n",
    "                    data['season_betrayal'] = last_s\n",
    "                    data['season_before_betrayal'] = (last_s-s)/0.5\n",
    "                    data['idx'] = idx\n",
    "                    result.append(data)\n",
    "                for m_bet in season['messages']['betrayer']:\n",
    "                    data = to_dict(m_bet)\n",
    "                    data['role'] = 'betrayer'\n",
    "                    data['season'] = s\n",
    "                    data['betrayal'] = betrayal\n",
    "                    data['season_betrayal'] = last_s\n",
    "                    data['season_before_betrayal'] = (last_s-s)/0.5\n",
    "                    data['idx'] = idx\n",
    "                    result.append(data)\n",
    "                            \n",
    "    return pd.DataFrame(result).set_index(['idx', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>all_words</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1906.5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, bot, ,, ., take, unit, war, retreat, di...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>2</td>\n",
       "      <td>[,, your, if, to, from, bot, retreat, stp, nwy...</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[armies, north, the, armies, on, ., your, with...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.932326</td>\n",
       "      <td>6</td>\n",
       "      <td>[,, your, sounds, against, to, next, moves, be...</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[?, going, for, ser, balance, a, to, of, give,...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>4</td>\n",
       "      <td>[as, ,, your, ser, to, appreciated, it, give, ...</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>[only, he, alb, ., forced, italy's, is, be, .,...</td>\n",
       "      <td>313</td>\n",
       "      <td>0.957072</td>\n",
       "      <td>19</td>\n",
       "      <td>[what, as, army, up, worth, lack, to, units, r...</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>[more, let, keep, we, side, we, don't, to, ., ...</td>\n",
       "      <td>146</td>\n",
       "      <td>0.832023</td>\n",
       "      <td>9</td>\n",
       "      <td>[army, up, to, retreat, something, more, keep,...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "idx season                                                              \n",
       "0   1906.5                   0                  0                   2   \n",
       "    1906.5                   1                  1                   4   \n",
       "    1906.5                   1                  2                   1   \n",
       "    1906.5                   4                  2                  13   \n",
       "    1906.5                   1                  3                   5   \n",
       "\n",
       "            n_requests                                     frequent_words  \\\n",
       "idx season                                                                  \n",
       "0   1906.5           1  [just, bot, ,, ., take, unit, war, retreat, di...   \n",
       "    1906.5           2  [armies, north, the, armies, on, ., your, with...   \n",
       "    1906.5           2  [?, going, for, ser, balance, a, to, of, give,...   \n",
       "    1906.5           8  [only, he, alb, ., forced, italy's, is, be, .,...   \n",
       "    1906.5           7  [more, let, keep, we, side, we, don't, to, ., ...   \n",
       "\n",
       "            n_words  politeness  n_sentences  \\\n",
       "idx season                                     \n",
       "0   1906.5       35    0.367200            2   \n",
       "    1906.5       77    0.932326            6   \n",
       "    1906.5       55    0.983373            4   \n",
       "    1906.5      313    0.957072           19   \n",
       "    1906.5      146    0.832023            9   \n",
       "\n",
       "                                                    all_words      role  \\\n",
       "idx season                                                                \n",
       "0   1906.5  [,, your, if, to, from, bot, retreat, stp, nwy...    victim   \n",
       "    1906.5  [,, your, sounds, against, to, next, moves, be...    victim   \n",
       "    1906.5  [as, ,, your, ser, to, appreciated, it, give, ...    victim   \n",
       "    1906.5  [what, as, army, up, worth, lack, to, units, r...    victim   \n",
       "    1906.5  [army, up, to, retreat, something, more, keep,...  betrayer   \n",
       "\n",
       "            betrayal  season_betrayal  season_before_betrayal  \n",
       "idx season                                                     \n",
       "0   1906.5      True           1909.5                     6.0  \n",
       "    1906.5      True           1909.5                     6.0  \n",
       "    1906.5      True           1909.5                     6.0  \n",
       "    1906.5      True           1909.5                     6.0  \n",
       "    1906.5      True           1909.5                     6.0  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessing(data)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In each season, potential betrayers send in average 1.627498001598721, with a maximum of 38 messages\n",
      "In each season, potential victims send in average 1.515587529976019, with a maximum of 28 messages\n"
     ]
    }
   ],
   "source": [
    "def get_nb_msg(data):\n",
    "    \"\"\"\n",
    "    Get the mean number of messages sent per season\n",
    "    \"\"\"\n",
    "    tab = []\n",
    "    for features in data[\"features\"]:\n",
    "        tab.append(len(features[0]))\n",
    "    return tab\n",
    "\n",
    "print(\"In each season, potential betrayers send in average {}, with a maximum of {} messages\".format(np.mean(get_nb_msg(data_betrayer)), np.max(get_nb_msg(data_betrayer))))\n",
    "print(\"In each season, potential victims send in average {}, with a maximum of {} messages\".format(np.mean(get_nb_msg(data_victim)), np.max(get_nb_msg(data_victim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_words(entry):\n",
    "    \"\"\"\n",
    "    get the set of lexicon words for each entry of the dataset\n",
    "    1 entry = 1 row = 1 set of messages\n",
    "    Can be improved\n",
    "    \"\"\"\n",
    "    for entries in entry[0]: #loop over the messages\n",
    "        # get the lexicon words\n",
    "        di_words = entries[\"lexicon_words\"]\n",
    "        tab_words = []\n",
    "        for key in di_words:\n",
    "            tab = di_words[key]\n",
    "            for words in tab:\n",
    "                word = words.split(' ')\n",
    "                for w in word:\n",
    "                    if w not in tab_words:\n",
    "                        tab_words.append(w)\n",
    "    return tab_words\n",
    "\n",
    "test = get_lexicon_words(data_victim[\"features\"][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season_before_betrayal\n",
       "1.0    343\n",
       "2.0    360\n",
       "3.0    314\n",
       "4.0    277\n",
       "5.0    231\n",
       "6.0    156\n",
       "7.0     94\n",
       "8.0     78\n",
       "9.0     42\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"betrayal\"]==True) & (df[\"role\"] == \"betrayer\")].reset_index().groupby('season_before_betrayal').count()['idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 367 instances of 1 season before betrayal, 379 instances of 2 seasons before betrayal etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting iminent betrayal - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider only messages that have been exchanged one season before the betrayal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>all_words</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[would, on, other, france, i, that, to, think,...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.951652</td>\n",
       "      <td>3</td>\n",
       "      <td>[great, basically, ,, stalemate, offer, if, to...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, the, the, ,, game, that, unless, on, wa...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.867535</td>\n",
       "      <td>2</td>\n",
       "      <td>[before, maybe, ,, way, if, ways, to, game, bu...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[france, for, see, now, would, did, just, what...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.464116</td>\n",
       "      <td>1</td>\n",
       "      <td>[what, they, see, lose, would, france, just, f...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[bla, not, of, always, a, if, goes, take, ,, t...</td>\n",
       "      <td>134</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>6</td>\n",
       "      <td>[as, always, up, to, worth, makes, so, really,...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[about, to, aeg, was, now, anyway, has, gre, b...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.218056</td>\n",
       "      <td>3</td>\n",
       "      <td>[what, ,, ser, was, to, am, from, he, alb, is,...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[war, after, and, which, that, into, may, you,...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.784303</td>\n",
       "      <td>3</td>\n",
       "      <td>[our, ,, into, am, to, next, then, have, think...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[another, game, ., we, i, ., think, 6, have, ....</td>\n",
       "      <td>18</td>\n",
       "      <td>0.690164</td>\n",
       "      <td>3</td>\n",
       "      <td>[so, have, up, set, far, game, another, 6, we,...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[vie, into, and, ., you, tun, also, ahead, sup...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.600676</td>\n",
       "      <td>2</td>\n",
       "      <td>[tun, support, move, you, ahead, and, go, into...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[and, through, ., sounds, enter, to, i'm, move...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.852749</td>\n",
       "      <td>4</td>\n",
       "      <td>[sounds, to, moves, perfect, through, but, aus...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[on, talk, ., write, ., and, if, more, perfect...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>3</td>\n",
       "      <td>[more, talk, on, ,, my, sounds, and, write, if...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  season  sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "0    0  1909.0                   0                  1                   2   \n",
       "1    0  1909.0                   0                  0                   2   \n",
       "2    0  1909.0                   0                  1                   0   \n",
       "3    3  1902.5                   0                  2                   4   \n",
       "4    3  1902.5                   1                  0                   2   \n",
       "5    4  1911.0                   0                  2                   1   \n",
       "6    4  1911.0                   0                  3                   0   \n",
       "7    4  1911.0                   1                  1                   0   \n",
       "8    5  1906.0                   3                  0                   1   \n",
       "9    8  1906.0                   1                  2                   0   \n",
       "\n",
       "   n_requests                                     frequent_words  n_words  \\\n",
       "0           2  [would, on, other, france, i, that, to, think,...       42   \n",
       "1           1  [know, the, the, ,, game, that, unless, on, wa...       51   \n",
       "2           1  [france, for, see, now, would, did, just, what...       14   \n",
       "3           4  [bla, not, of, always, a, if, goes, take, ,, t...      134   \n",
       "4           2  [about, to, aeg, was, now, anyway, has, gre, b...       62   \n",
       "5           2  [war, after, and, which, that, into, may, you,...       44   \n",
       "6           0  [another, game, ., we, i, ., think, 6, have, ....       18   \n",
       "7           0  [vie, into, and, ., you, tun, also, ahead, sup...       17   \n",
       "8           1  [and, through, ., sounds, enter, to, i'm, move...       33   \n",
       "9           1  [on, talk, ., write, ., and, if, more, perfect...       20   \n",
       "\n",
       "   politeness  n_sentences                                          all_words  \\\n",
       "0    0.951652            3  [great, basically, ,, stalemate, offer, if, to...   \n",
       "1    0.867535            2  [before, maybe, ,, way, if, ways, to, game, bu...   \n",
       "2    0.464116            1  [what, they, see, lose, would, france, just, f...   \n",
       "3    0.855036            6  [as, always, up, to, worth, makes, so, really,...   \n",
       "4    0.218056            3  [what, ,, ser, was, to, am, from, he, alb, is,...   \n",
       "5    0.784303            3  [our, ,, into, am, to, next, then, have, think...   \n",
       "6    0.690164            3  [so, have, up, set, far, game, another, 6, we,...   \n",
       "7    0.600676            2  [tun, support, move, you, ahead, and, go, into...   \n",
       "8    0.852749            4  [sounds, to, moves, perfect, through, but, aus...   \n",
       "9    0.480527            3  [more, talk, on, ,, my, sounds, and, write, if...   \n",
       "\n",
       "       role  betrayal  season_betrayal  season_before_betrayal  \n",
       "0  betrayer      True           1909.5                     1.0  \n",
       "1  betrayer      True           1909.5                     1.0  \n",
       "2  betrayer      True           1909.5                     1.0  \n",
       "3  betrayer     False           1903.0                     1.0  \n",
       "4  betrayer     False           1903.0                     1.0  \n",
       "5  betrayer     False           1911.5                     1.0  \n",
       "6  betrayer     False           1911.5                     1.0  \n",
       "7  betrayer     False           1911.5                     1.0  \n",
       "8  betrayer      True           1906.5                     1.0  \n",
       "9  betrayer      True           1906.5                     1.0  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = df[(df[\"season_before_betrayal\"]==1) & (df[\"role\"] == \"betrayer\")].reset_index().copy()\n",
    "data_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>all_words</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[would, on, other, france, i, that, to, think,...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.951652</td>\n",
       "      <td>3</td>\n",
       "      <td>[great, basically, ,, stalemate, offer, if, to...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, the, the, ,, game, that, unless, on, wa...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.867535</td>\n",
       "      <td>2</td>\n",
       "      <td>[before, maybe, ,, way, if, ways, to, game, bu...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[france, for, see, now, would, did, just, what...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.464116</td>\n",
       "      <td>1</td>\n",
       "      <td>[what, they, see, lose, would, france, just, f...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[bla, not, of, always, a, if, goes, take, ,, t...</td>\n",
       "      <td>134</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>6</td>\n",
       "      <td>[as, always, up, to, worth, makes, so, really,...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[about, to, aeg, was, now, anyway, has, gre, b...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.218056</td>\n",
       "      <td>3</td>\n",
       "      <td>[what, ,, ser, was, to, am, from, he, alb, is,...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[war, after, and, which, that, into, may, you,...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.784303</td>\n",
       "      <td>3</td>\n",
       "      <td>[our, ,, into, am, to, next, then, have, think...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[another, game, ., we, i, ., think, 6, have, ....</td>\n",
       "      <td>18</td>\n",
       "      <td>0.690164</td>\n",
       "      <td>3</td>\n",
       "      <td>[so, have, up, set, far, game, another, 6, we,...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[vie, into, and, ., you, tun, also, ahead, sup...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.600676</td>\n",
       "      <td>2</td>\n",
       "      <td>[tun, support, move, you, ahead, and, go, into...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[and, through, ., sounds, enter, to, i'm, move...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.852749</td>\n",
       "      <td>4</td>\n",
       "      <td>[sounds, to, moves, perfect, through, but, aus...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[on, talk, ., write, ., and, if, more, perfect...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>3</td>\n",
       "      <td>[more, talk, on, ,, my, sounds, and, write, if...</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  season  sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "0    0  1909.0                   0                  1                   2   \n",
       "1    0  1909.0                   0                  0                   2   \n",
       "2    0  1909.0                   0                  1                   0   \n",
       "3    3  1902.5                   0                  2                   4   \n",
       "4    3  1902.5                   1                  0                   2   \n",
       "5    4  1911.0                   0                  2                   1   \n",
       "6    4  1911.0                   0                  3                   0   \n",
       "7    4  1911.0                   1                  1                   0   \n",
       "8    5  1906.0                   3                  0                   1   \n",
       "9    8  1906.0                   1                  2                   0   \n",
       "\n",
       "   n_requests                                     frequent_words  n_words  \\\n",
       "0           2  [would, on, other, france, i, that, to, think,...       42   \n",
       "1           1  [know, the, the, ,, game, that, unless, on, wa...       51   \n",
       "2           1  [france, for, see, now, would, did, just, what...       14   \n",
       "3           4  [bla, not, of, always, a, if, goes, take, ,, t...      134   \n",
       "4           2  [about, to, aeg, was, now, anyway, has, gre, b...       62   \n",
       "5           2  [war, after, and, which, that, into, may, you,...       44   \n",
       "6           0  [another, game, ., we, i, ., think, 6, have, ....       18   \n",
       "7           0  [vie, into, and, ., you, tun, also, ahead, sup...       17   \n",
       "8           1  [and, through, ., sounds, enter, to, i'm, move...       33   \n",
       "9           1  [on, talk, ., write, ., and, if, more, perfect...       20   \n",
       "\n",
       "   politeness  n_sentences                                          all_words  \\\n",
       "0    0.951652            3  [great, basically, ,, stalemate, offer, if, to...   \n",
       "1    0.867535            2  [before, maybe, ,, way, if, ways, to, game, bu...   \n",
       "2    0.464116            1  [what, they, see, lose, would, france, just, f...   \n",
       "3    0.855036            6  [as, always, up, to, worth, makes, so, really,...   \n",
       "4    0.218056            3  [what, ,, ser, was, to, am, from, he, alb, is,...   \n",
       "5    0.784303            3  [our, ,, into, am, to, next, then, have, think...   \n",
       "6    0.690164            3  [so, have, up, set, far, game, another, 6, we,...   \n",
       "7    0.600676            2  [tun, support, move, you, ahead, and, go, into...   \n",
       "8    0.852749            4  [sounds, to, moves, perfect, through, but, aus...   \n",
       "9    0.480527            3  [more, talk, on, ,, my, sounds, and, write, if...   \n",
       "\n",
       "       role  betrayal  season_betrayal  season_before_betrayal  \n",
       "0  betrayer      True           1909.5                     1.0  \n",
       "1  betrayer      True           1909.5                     1.0  \n",
       "2  betrayer      True           1909.5                     1.0  \n",
       "3  betrayer     False           1903.0                     1.0  \n",
       "4  betrayer     False           1903.0                     1.0  \n",
       "5  betrayer     False           1911.5                     1.0  \n",
       "6  betrayer     False           1911.5                     1.0  \n",
       "7  betrayer     False           1911.5                     1.0  \n",
       "8  betrayer      True           1906.5                     1.0  \n",
       "9  betrayer      True           1906.5                     1.0  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_1[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', 'n_sentences', 'politeness']].values, 1*data_1['betrayal'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "def normalize(x):\n",
    "    x_temp = x.copy()\n",
    "    for i in range(len(x[0, :])-1):\n",
    "        m = np.mean(x[:, i])\n",
    "        s = np.std(x[:, i])\n",
    "        x_temp[:, i] -= m\n",
    "        x_temp[:, i] /= s\n",
    "    return x_temp\n",
    "\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An intuition for finding the best model / configuration of features for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def log_reg_pipeline(data, features, normalize_=True, print_=True):\n",
    "    X, Y = data[features].values, 1*data['betrayal'].values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "    if normalize_:\n",
    "        x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "        clf = LogisticRegression(random_state=0).fit(x_train_nor, y_train)\n",
    "        score = clf.score(x_test_nor, y_test)\n",
    "        if print_:\n",
    "            print(\"score on the test set : {}\".format(score))\n",
    "        return clf, score\n",
    "    else:\n",
    "        clf = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
    "        score = clf.score(x_test, y_test)\n",
    "        if print_:\n",
    "            print(\"score on the test set : {}\".format(score))\n",
    "        return clf, score\n",
    "\n",
    "features = [\n",
    "            ['politeness'],\n",
    "            ['n_words', 'politeness'],\n",
    "            ['n_requests', 'n_words', 'politeness'],\n",
    "            ['sentiment_positive', 'n_requests', 'n_words', 'politeness'],\n",
    "            ['sentiment_positive', 'sentiment_neutral', 'n_requests', 'n_words', 'politeness'],\n",
    "           ['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', 'politeness']]\n",
    "\n",
    "ax1 = []\n",
    "ax2 = []\n",
    "ax3 = []\n",
    "\n",
    "for feature in features:\n",
    "    _, score = log_reg_pipeline(data_1, feature, False, False)\n",
    "    _, score_nor = log_reg_pipeline(data_1, feature, True, False)\n",
    "    ax1.append(len(feature))\n",
    "    ax2.append(score)\n",
    "    ax3.append(score_nor)\n",
    "    \n",
    "_ = plt.plot(ax1, ax2)\n",
    "_ = plt.plot(ax1, ax3)\n",
    "_ = plt.legend(['not normalized', 'normalized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The feed forward model + some preprocessing to understand better the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = df.copy()\n",
    "\n",
    "# Now train just for the betrayer role for changes, although we could add data for both?\n",
    "features_data = features_data[features_data['role'] == 'betrayer']\n",
    "features_data = features_data[features_data['betrayal'] == True]\n",
    "\n",
    "features_data = features_data.drop(columns=['frequent_words', 'all_words', 'season_betrayal', 'role', 'betrayal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggreagted_features_per_season = features_data.groupby(['idx', 'season'], as_index=False).aggregate({\n",
    "    'sentiment_positive': 'mean',\n",
    "    'sentiment_neutral': 'mean',\n",
    "    'sentiment_negative': 'mean',\n",
    "    'n_requests': 'mean',\n",
    "    'n_words': 'sum',\n",
    "    'politeness': 'mean',\n",
    "    'n_sentences': 'sum',\n",
    "    'season_before_betrayal': 'min'\n",
    "})\n",
    "\n",
    "X = aggreagted_features_per_season[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',\n",
    "       'n_requests', 'n_words', 'politeness', 'n_sentences']]\n",
    "Y = (aggreagted_features_per_season['season_before_betrayal'] == 1.0).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>489</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>280</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>333</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>449</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>0.899161</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_positive  sentiment_neutral  sentiment_negative  n_requests  \\\n",
       "0            1.333333           1.333333            1.500000    3.666667   \n",
       "1            0.142857           0.857143            1.285714    1.285714   \n",
       "2            2.000000           2.500000            2.000000    5.500000   \n",
       "3            1.800000           0.800000            2.200000    3.200000   \n",
       "4            1.000000           1.000000            1.000000    2.000000   \n",
       "\n",
       "   n_words  politeness  n_sentences  \n",
       "0      489    0.803328           25  \n",
       "1      280    0.560083           16  \n",
       "2      333    0.982703           13  \n",
       "3      449    0.748802           24  \n",
       "4       78    0.899161            6  "
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def matthews_corr_coef(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TP, FP, FN, TN = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = np.sum(y_pred == y_true) / n\n",
    "    \n",
    "    mmc = matthews_corr_coef(y_true, y_pred)\n",
    "    \n",
    "    return {'f1': np.round(f1, decimals=3),\n",
    "            'mmc': np.round(mmc, decimals=6),\n",
    "            'acc': np.round(accuracy, decimals=3),\n",
    "            'precision': np.round(precision, decimals=3),\n",
    "            'recall': np.round(recall, decimals=3),\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'tn': tn,\n",
    "            'fn': fn\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6207865168539326, 1: 2.5697674418604652}"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To address imbalance, compute the weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f1': 0.368, 'mmc': 0.106161, 'acc': 72.105, 'precision': 0.233, 'recall': 0.421, 'tp': 1736, 'fp': 5712, 'tn': 7854, 'fn': 2387}\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "FEATURES_NUM = X.shape[-1]\n",
    "\n",
    "def get_feed_forward_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=512, activation='tanh', kernel_regularizer=L2(1e-5), bias_regularizer=L2(1e-5), input_shape=(FEATURES_NUM,)))\n",
    "    model.add(Dense(units=256))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy')    \n",
    "    return model\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def local_seed(seed):\n",
    "    state = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        np.random.set_state(state)\n",
    "        tf.random.set_seed(np.random.randint(1000))\n",
    "        \n",
    "def train_and_predict_feed_forward_model(x_train, x_test, y_train, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n",
    "\n",
    "    with local_seed(10):\n",
    "        model = get_feed_forward_model()\n",
    "        result = model.fit(\n",
    "            x_train, \n",
    "            y_train, \n",
    "            batch_size = BATCH_SIZE, \n",
    "            epochs=10,\n",
    "            callbacks=[es],\n",
    "            validation_data=(x_test, y_test),\n",
    "            class_weight=class_weights, verbose=0)\n",
    "\n",
    "    y_pred = model.predict_step(x_test).numpy()\n",
    "    y_pred_bool = y_pred > 0.5\n",
    "    return evaluate_model(y_test, y_pred_bool)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 10, test_size=0.2)\n",
    "# scaler = StandardScaler()\n",
    "# x_train = scaler.fit_transform(x_train)\n",
    "# x_test = scaler.transform(x_test)\n",
    "\n",
    "print(train_and_predict_feed_forward_model(x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model Feedforward..\n",
      "F1-Score: 95% confidence interval 0.166 and 0.515\n",
      "Matthews Corr Coef: 95% confidence interval -0.220 and 0.335\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import sem\n",
    "\n",
    "def confidence_interval(confidence, values):\n",
    "    lower_p = (1.0 - confidence)/2\n",
    "    upper_p = 1 - (1.0 - confidence)/2\n",
    "    return np.percentile(values, lower_p * 100), np.percentile(values, upper_p * 100)\n",
    "\n",
    "def bootstrap_model_prediction(train_and_predict_fn, n_iterations, stratify, model_name=\"\"):\n",
    "    print('Training model {}..'.format(model_name))\n",
    "    \n",
    "    f1_scores = []\n",
    "    mmc_scores = []\n",
    "    for i in range(n_iterations):\n",
    "        # Split data before resampling\n",
    "        \n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=10, shuffle=True)\n",
    "\n",
    "        # Resample train and test data, stratifying on y (resulting in equal number of 0 and 1 labels)\n",
    "        if stratify:\n",
    "            x_train, y_train = resample(x_train, y_train, replace=True, stratify=y_train)\n",
    "        else:\n",
    "            x_train, y_train = resample(x_train, y_train, replace=True)\n",
    "            \n",
    "        x_test, y_test = resample( x_test, y_test, replace=True)\n",
    "                \n",
    "        scores = train_and_predict_fn(x_train, x_test, y_train, y_test)\n",
    "        f1_scores.append(scores['f1'])\n",
    "        mmc_scores.append(scores['mmc'])\n",
    "    f1_score_low, f1_score_upper = confidence_interval(0.95, f1_scores)\n",
    "    mmc_score_low, mmc_score_upper = confidence_interval(0.95, np.nan_to_num(mmc_scores))\n",
    "\n",
    "    print('F1-Score: 95%% confidence interval %.3f and %.3f' % (f1_score_low, f1_score_upper))\n",
    "    print('Matthews Corr Coef: 95%% confidence interval %.3f and %.3f' % (mmc_score_low, mmc_score_upper))\n",
    "    \n",
    "    return [np.mean(f1_scores), sem(f1_scores)], [np.mean(mmc_scores), sem(mmc_scores)]\n",
    "\n",
    "# Desirable to do the same with all models\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_feed_forward_model, 20, stratify=True, model_name=\"Feedforward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: Add all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1s = np.array([f1_output, f1_output, f1_output, f1_output])\n",
    "mmcs = np.array([mmc_output, mmc_output, mmc_output, mmc_output])\n",
    "model_names = [\"Feedforward\", \"Feedforwardp\", \"Feedforwardc\", \"Feedforwarda\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.barh(model_names, f1s[:, 0], xerr=f1s[:,1], capsize=4)\n",
    "plt.title(\"Score\")\n",
    "plt.xlabel(\"f1\".capitalize())\n",
    "plt.ylabel(\"Model\")\n",
    "plt.show()\n",
    "\n",
    "plt.barh(model_names, mmcs[:, 0], xerr=mmcs[:,1], capsize=4)\n",
    "plt.title(\"Score\")\n",
    "plt.xlabel(\"mmc\".capitalize())\n",
    "plt.ylabel(\"Model\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model using word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/ina/.local/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: click in /home/ina/.local/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in /home/ina/.local/lib/python3.7/site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: tqdm in /home/ina/.local/lib/python3.7/site-packages (from nltk) (4.38.0)\n",
      "Requirement already satisfied: joblib in /home/ina/.local/lib/python3.7/site-packages (from nltk) (0.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = df.copy()\n",
    "\n",
    "# Now train just for the betrayer role for changes, although we could add data for both?\n",
    "features_data = features_data[features_data['role'] == 'betrayer']\n",
    "features_data = features_data[features_data['betrayal'] == True]\n",
    "\n",
    "features_data = features_data.drop(columns=['frequent_words', 'season_betrayal', 'role', 'betrayal'])\n",
    "\n",
    "aggreagted_features_per_season = features_data.groupby(['idx', 'season'], as_index=False).aggregate({\n",
    "    'sentiment_positive': 'mean',\n",
    "    'sentiment_neutral': 'mean',\n",
    "    'sentiment_negative': 'mean',\n",
    "    'n_requests': 'mean',\n",
    "    'n_words': 'sum',\n",
    "    'politeness': 'mean',\n",
    "    'n_sentences': 'sum',\n",
    "    'season_before_betrayal': 'min',\n",
    "    'all_words': 'sum'\n",
    "})\n",
    "\n",
    "X = aggreagted_features_per_season[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',\n",
    "       'n_requests', 'n_words', 'politeness', 'n_sentences', 'all_words']]\n",
    "Y = (aggreagted_features_per_season['season_before_betrayal'] == 1.0).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>489</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>25</td>\n",
       "      <td>[army, up, to, retreat, something, more, keep,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>280</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>16</td>\n",
       "      <td>[basically, ,, turkey, if, to, game, wonder, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>333</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>13</td>\n",
       "      <td>[year, what, so that, how, into, to, hi, next,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>449</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>24</td>\n",
       "      <td>[strategy, early, a, out, they, ,, move, hope,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>0.899161</td>\n",
       "      <td>6</td>\n",
       "      <td>[as, ,, turkey, how, am, to, we've, next, made...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>658</th>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>409</td>\n",
       "      <td>0.395679</td>\n",
       "      <td>26</td>\n",
       "      <td>[army, up, into, to, push, nwy, so, really, ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>659</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>71</td>\n",
       "      <td>0.785834</td>\n",
       "      <td>5</td>\n",
       "      <td>[near, our, nth, ,, your, nwg, to, then, but, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>124</td>\n",
       "      <td>0.354644</td>\n",
       "      <td>11</td>\n",
       "      <td>[t, as, ,, why, case, wrote, both, might, agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>86</td>\n",
       "      <td>0.510478</td>\n",
       "      <td>9</td>\n",
       "      <td>[as, nth, your, to, taking, it, at, the, med, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>112</td>\n",
       "      <td>0.352043</td>\n",
       "      <td>13</td>\n",
       "      <td>[,, start, too, from, more, r, take, have, agr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>663 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentiment_positive  sentiment_neutral  sentiment_negative  n_requests  \\\n",
       "0              1.333333           1.333333            1.500000    3.666667   \n",
       "1              0.142857           0.857143            1.285714    1.285714   \n",
       "2              2.000000           2.500000            2.000000    5.500000   \n",
       "3              1.800000           0.800000            2.200000    3.200000   \n",
       "4              1.000000           1.000000            1.000000    2.000000   \n",
       "..                  ...                ...                 ...         ...   \n",
       "658            1.666667           3.000000            4.000000    3.000000   \n",
       "659            2.000000           1.000000            2.000000    2.000000   \n",
       "660            0.500000           1.500000            3.500000    2.000000   \n",
       "661            0.500000           0.500000            1.250000    1.000000   \n",
       "662            0.500000           1.500000            4.500000    2.500000   \n",
       "\n",
       "     n_words  politeness  n_sentences  \\\n",
       "0        489    0.803328           25   \n",
       "1        280    0.560083           16   \n",
       "2        333    0.982703           13   \n",
       "3        449    0.748802           24   \n",
       "4         78    0.899161            6   \n",
       "..       ...         ...          ...   \n",
       "658      409    0.395679           26   \n",
       "659       71    0.785834            5   \n",
       "660      124    0.354644           11   \n",
       "661       86    0.510478            9   \n",
       "662      112    0.352043           13   \n",
       "\n",
       "                                             all_words  \n",
       "0    [army, up, to, retreat, something, more, keep,...  \n",
       "1    [basically, ,, turkey, if, to, game, wonder, a...  \n",
       "2    [year, what, so that, how, into, to, hi, next,...  \n",
       "3    [strategy, early, a, out, they, ,, move, hope,...  \n",
       "4    [as, ,, turkey, how, am, to, we've, next, made...  \n",
       "..                                                 ...  \n",
       "658  [army, up, into, to, push, nwy, so, really, ha...  \n",
       "659  [near, our, nth, ,, your, nwg, to, then, but, ...  \n",
       "660  [t, as, ,, why, case, wrote, both, might, agai...  \n",
       "661  [as, nth, your, to, taking, it, at, the, med, ...  \n",
       "662  [,, start, too, from, more, r, take, have, agr...  \n",
       "\n",
       "[663 rows x 8 columns]"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['army', 'up', 'to', 'retreat', 'something', 'more', 'keep', 'yet', 'until', 'one', 'the', 'prevent', 'there', 'a', 'my', \"don't\", 'can', 'instead', 'will', 'likely', 'good', 'they', 'doing', 'going', 'ground', \"didn't\", 'might', 'troops', 'still', 'of', 'support', 'you', 'sure', 'i', 'ally', 'mos', 'by', 'crazy', 'losing', 'before', 'run', ',', 'your', 'turkey', 'am', 'it', 'most', 'be', 'but', 'have']\n"
     ]
    }
   ],
   "source": [
    "# Words combined look like this\n",
    "print(X['all_words'][0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# We notice multiple stop words need to be removed, as well as contractions\n",
    "def clean_words(X):\n",
    "    new_words = []\n",
    "    for word in X:\n",
    "        # We consider contractions not very valuable, as well as very short words/individual letters\n",
    "        if len(word) <= 2 or '\\'' in word:\n",
    "            continue\n",
    "        \n",
    "        # Removing stop words\n",
    "        if word in stopwords.words('english'):\n",
    "            continue\n",
    "        \n",
    "        new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "X['all_words'] = X['all_words'].map(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max number of words per season 938\n",
      "Mean number of words per season 81.11915535444948\n",
      "Min number of words per season 0\n",
      "Number of unique words are 2146\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "total_words_per_season = [len(X['all_words'][i]) for i in range(X.shape[0])]\n",
    "print('Max number of words per season', np.max(total_words_per_season))\n",
    "print('Mean number of words per season', np.mean(total_words_per_season))\n",
    "print('Min number of words per season', np.min(total_words_per_season))\n",
    "\n",
    "all_words = []\n",
    "for idx in range(X.shape[0]):\n",
    "    all_words = all_words + X['all_words'][idx]\n",
    "print('Number of unique words are', len(set(all_words)))\n",
    "\n",
    "plt.plot(total_words_per_season)\n",
    "plt.show()\n",
    "\n",
    "# Based on the following, we decide to pad/truncate to max 200 words and use an embedding matrix of 2140 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings containing 1193514 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>all_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>489</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>25</td>\n",
       "      <td>[1036, 377, 244, 10, 1182, 384, 1882, 1199, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>280</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>16</td>\n",
       "      <td>[1458, 1925, 276, 1218, 1937, 384, 848, 881, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>333</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>13</td>\n",
       "      <td>[108, 1872, 384, 253, 523, 23, 1887, 675, 1601...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>449</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>24</td>\n",
       "      <td>[135, 1260, 1559, 1550, 1925, 399, 819, 575, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>0.899161</td>\n",
       "      <td>6</td>\n",
       "      <td>[1925, 1872, 1618, 2022, 1559, 983, 48, 1293, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_positive  sentiment_neutral  sentiment_negative  n_requests  \\\n",
       "0            1.333333           1.333333            1.500000    3.666667   \n",
       "1            0.142857           0.857143            1.285714    1.285714   \n",
       "2            2.000000           2.500000            2.000000    5.500000   \n",
       "3            1.800000           0.800000            2.200000    3.200000   \n",
       "4            1.000000           1.000000            1.000000    2.000000   \n",
       "\n",
       "   n_words  politeness  n_sentences  \\\n",
       "0      489    0.803328           25   \n",
       "1      280    0.560083           16   \n",
       "2      333    0.982703           13   \n",
       "3      449    0.748802           24   \n",
       "4       78    0.899161            6   \n",
       "\n",
       "                                           all_words  \n",
       "0  [1036, 377, 244, 10, 1182, 384, 1882, 1199, 25...  \n",
       "1  [1458, 1925, 276, 1218, 1937, 384, 848, 881, 4...  \n",
       "2  [108, 1872, 384, 253, 523, 23, 1887, 675, 1601...  \n",
       "3  [135, 1260, 1559, 1550, 1925, 399, 819, 575, 1...  \n",
       "4  [1925, 1872, 1618, 2022, 1559, 983, 48, 1293, ...  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 25\n",
    "MAX_WORDS = 2147\n",
    "MAX_LEN = 200\n",
    "\n",
    "def read_low_dim_glove_embeddings(filename):\n",
    "    embeddings = {}\n",
    "    \n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        embeddings[word] = np.asarray(values[1:], dtype='float32')\n",
    "    f.close()\n",
    "    \n",
    "    print('Embeddings containing %s word vectors.' % len(embeddings))\n",
    "    return embeddings\n",
    "\n",
    "embeddings = read_low_dim_glove_embeddings('glove.twitter.27B.25d.txt')\n",
    "embedding_matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM))\n",
    "\n",
    "word_to_idx = {}\n",
    "idx = 0\n",
    "for word in list(set(all_words)):\n",
    "    embedding_vector = embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "        word_to_idx[word] = idx\n",
    "    idx = idx + 1\n",
    "\n",
    "def convert_word_to_id(words):\n",
    "    values = [word_to_idx[w] for w in words if w in word_to_idx]\n",
    "    \n",
    "    return values\n",
    "\n",
    "X['all_words'] = X['all_words'].map(convert_word_to_id)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 25)      53675       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 7)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           23040       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           512         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 128)          0           lstm[0][0]                       \n",
      "                                                                 dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 32)           4128        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            33          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 81,388\n",
      "Trainable params: 27,713\n",
      "Non-trainable params: 53,675\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_embedding_submodel():\n",
    "    inputs = tf.keras.layers.Input(shape=(MAX_LEN))\n",
    "    embedding = tf.keras.layers.Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    lstm = tf.keras.layers.LSTM(64, activation='relu', kernel_regularizer=L2(1e-5), bias_regularizer=L2(1e-5))(embedding)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=lstm)\n",
    "    return model\n",
    "\n",
    "def get_numerical_submodel():\n",
    "    inputs = tf.keras.layers.Input(shape=(7))\n",
    "    dense = tf.keras.layers.Dense(64, kernel_regularizer=L2(1e-5), bias_regularizer=L2(1e-5))(inputs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=dense)\n",
    "    return model\n",
    "\n",
    "def get_combined_model():\n",
    "    model_embeddings = get_embedding_submodel()\n",
    "    model_numerical = get_numerical_submodel()    \n",
    "    combined = tf.keras.layers.concatenate([model_embeddings.output, model_numerical.output])\n",
    "    dense = tf.keras.layers.Dense(32, activation='linear')(combined)\n",
    "    dropout = tf.keras.layers.Dropout(0.4)(combined)\n",
    "    final_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[model_embeddings.input, model_numerical.input], outputs=final_output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0005))\n",
    "    return model\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_combined_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model Fusion model..\n",
      "F1-Score: 95% confidence interval 0.050 and 0.463\n",
      "Matthews Corr Coef: 95% confidence interval -0.233 and 0.202\n",
      "Average f1 0.28915\n",
      "Average mmc -0.001656649999999997\n"
     ]
    }
   ],
   "source": [
    "def get_numerical_model_x_inputs(x_train, x_test):\n",
    "    # Inputs for the numerical model\n",
    "    numerical_model_features = ['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',\n",
    "           'n_requests', 'n_words', 'politeness', 'n_sentences']\n",
    "    x_train = x_train[numerical_model_features]\n",
    "    x_test = x_test[numerical_model_features]\n",
    "\n",
    "    # Standardization of numerical features\n",
    "    scaler = StandardScaler()\n",
    "    x_train = scaler.fit_transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "    \n",
    "    return x_train, x_test\n",
    "\n",
    "def get_embeddings_model_x_inputs(x_train, x_test):\n",
    "    x_train = x_train['all_words']\n",
    "    x_test = x_test['all_words']\n",
    "    \n",
    "    x_train = tf.keras.preprocessing.sequence.pad_sequences(list(x_train), maxlen=MAX_LEN, padding='post')\n",
    "    x_test = tf.keras.preprocessing.sequence.pad_sequences(list(x_test), maxlen=MAX_LEN, padding='post')\n",
    "    return x_train, x_test\n",
    "\n",
    "def train_and_predict_fusion_model(x_train, x_test, y_train, y_test):\n",
    "    x_train_1, x_test_1 = get_embeddings_model_x_inputs(x_train, x_test)\n",
    "    x_train_2, x_test_2 = get_numerical_model_x_inputs(x_train, x_test)\n",
    "\n",
    "    with local_seed(10):\n",
    "        tf.keras.backend.clear_session()\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=3)\n",
    "\n",
    "        model = get_combined_model()\n",
    "        result = model.fit({'input_1': x_train_1, 'input_2': x_train_2},\n",
    "            y_train, \n",
    "            batch_size = 64, \n",
    "            epochs=10,\n",
    "            callbacks=[es],\n",
    "            validation_data=({'input_1': x_test_1, 'input_2': x_test_2}, y_test),\n",
    "            class_weight=class_weights, verbose=0)\n",
    "\n",
    "    y_pred = model.predict_step({'input_1': x_test_1, 'input_2': x_test_2}).numpy()\n",
    "    y_pred_bool = y_pred > 0.5\n",
    "    return evaluate_model(y_test, y_pred_bool)\n",
    "\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_fusion_model, 20, stratify=True, model_name=\"Fusion model\")\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 10, test_size=0.2, shuffle=True)\n",
    "# train_and_predict_fusion_model(x_train, x_test, y_train, y_test)\n",
    "print('Average f1', f1_output[0])\n",
    "print('Average mmc', mmc_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model Fusion model..\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7794 - val_loss: 0.6975\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 77ms/step - loss: 0.6942 - val_loss: 0.6335\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.7087 - val_loss: 0.6566\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.7040 - val_loss: 0.6708\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.6814 - val_loss: 0.6700\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.6922 - val_loss: 0.6695\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.6775 - val_loss: 0.6748\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6750 - val_loss: 0.6707\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6709 - val_loss: 0.6721\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6799 - val_loss: 0.6653\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6742 - val_loss: 0.6605\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.6653 - val_loss: 0.6521\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.6692 - val_loss: 0.6478\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.6804 - val_loss: 0.6507\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.6679 - val_loss: 0.6574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/ina/.local/lib/python3.7/site-packages/ipykernel_launcher.py:7: RuntimeWarning: invalid value encountered in true_divide\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.7703 - val_loss: 0.6515\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.7506 - val_loss: 0.6612\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.7098 - val_loss: 0.6819\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7022 - val_loss: 0.7001\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7022 - val_loss: 0.6994\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7077 - val_loss: 0.7025\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.7206 - val_loss: 0.7017\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.7014 - val_loss: 0.7073\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.7055 - val_loss: 0.7168\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.7018 - val_loss: 0.7010\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6909 - val_loss: 0.6724\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6965 - val_loss: 0.6913\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 85ms/step - loss: 0.6959 - val_loss: 0.7030\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 134ms/step - loss: 0.6937 - val_loss: 0.6930\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.6921 - val_loss: 0.6971\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.8048 - val_loss: 0.7208\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7391 - val_loss: 0.6882\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.7181 - val_loss: 0.6911\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.7157 - val_loss: 0.7085\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.7295 - val_loss: 0.7178\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.6994 - val_loss: 0.7025\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.7062 - val_loss: 0.6916\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 114ms/step - loss: 0.6747 - val_loss: 0.6689\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.7021 - val_loss: 0.6943\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 0.7057 - val_loss: 0.6809\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.7040 - val_loss: 0.6454\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6835 - val_loss: 0.6629\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.7007 - val_loss: 0.6714\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6870 - val_loss: 0.6843\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7019 - val_loss: 0.6739\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 123ms/step - loss: 0.7791 - val_loss: 0.6471\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7528 - val_loss: 0.6573\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.7078 - val_loss: 0.6829\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.7092 - val_loss: 0.6881\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.6928 - val_loss: 0.6792\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 120ms/step - loss: 0.7018 - val_loss: 0.6687\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.6957 - val_loss: 0.6739\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6977 - val_loss: 0.6829\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6883 - val_loss: 0.6849\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6958 - val_loss: 0.6937\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.7020 - val_loss: 0.6914\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7101 - val_loss: 0.6654\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7002 - val_loss: 0.6978\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.6882 - val_loss: 0.6976\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6938 - val_loss: 0.6804\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 125ms/step - loss: 0.7993 - val_loss: 0.6854\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 78ms/step - loss: 0.7254 - val_loss: 0.6584\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.7098 - val_loss: 0.6672\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6798 - val_loss: 0.6763\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.6961 - val_loss: 0.6773\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 0.6974 - val_loss: 0.6783\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 123ms/step - loss: 0.6998 - val_loss: 0.6761\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 0.6770 - val_loss: 0.6690\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.6740 - val_loss: 0.6632\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 0.6803 - val_loss: 0.6606\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.6786 - val_loss: 0.6535\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 30052.0410 - val_loss: 0.6345\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 115ms/step - loss: 0.6655 - val_loss: 0.6654\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6754 - val_loss: 0.6759\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.6741 - val_loss: 0.6805\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 123ms/step - loss: 0.7734 - val_loss: 0.6511\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6895 - val_loss: 0.6195\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6870 - val_loss: 0.6611\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6734 - val_loss: 0.6611\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6857 - val_loss: 0.6654\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6735 - val_loss: 0.6413\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.6658 - val_loss: 0.6169\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6721 - val_loss: 0.6443\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6612 - val_loss: 0.6417\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.6535 - val_loss: 0.6318\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6519 - val_loss: 0.5943\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.6577 - val_loss: 0.6181\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.6379 - val_loss: 0.6231\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 0.6494 - val_loss: 0.6210\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 121ms/step - loss: 0.6546 - val_loss: 510910.8750\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 126ms/step - loss: 0.8166 - val_loss: 0.7295\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7990 - val_loss: 0.7465\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7443 - val_loss: 0.7431\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7603 - val_loss: 0.7513\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7498 - val_loss: 0.7482\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.7571 - val_loss: 0.7414\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7504 - val_loss: 0.7234\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7250 - val_loss: 0.7072\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7418 - val_loss: 0.7373\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7243 - val_loss: 0.7226\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.7242 - val_loss: 0.7063\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7260 - val_loss: 0.7381\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7268 - val_loss: 0.7339\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7299 - val_loss: 0.7019\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7150 - val_loss: 0.7014\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 124ms/step - loss: 0.8223 - val_loss: 0.7146\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7734 - val_loss: 0.7281\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7407 - val_loss: 0.7303\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7566 - val_loss: 0.7141\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.7372 - val_loss: 0.6970\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.7363 - val_loss: 0.7147\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7234 - val_loss: 0.7070\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7208 - val_loss: 0.7251\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7329 - val_loss: 0.7324\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7210 - val_loss: 0.7202\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7217 - val_loss: 0.7068\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7175 - val_loss: 0.6986\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7209 - val_loss: 0.7199\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 0.7091 - val_loss: 0.7249\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.7165 - val_loss: 0.7005\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 0.7855 - val_loss: 0.6741\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7031 - val_loss: 0.6541\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6675 - val_loss: 0.6591\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6650 - val_loss: 0.6700\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6703 - val_loss: 0.6732\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6731 - val_loss: 0.6652\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6451 - val_loss: 0.6700\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.6235 - val_loss: 0.6836\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 6568521216.0000 - val_loss: 0.6733\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.6456 - val_loss: 0.6790\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.6407 - val_loss: 0.6817\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.6479 - val_loss: 0.6827\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.6401 - val_loss: 0.6825\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.6449 - val_loss: 0.6819\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6508 - val_loss: 0.6812\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 128ms/step - loss: 0.7817 - val_loss: 0.6863\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7656 - val_loss: 0.6938\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.7218 - val_loss: 0.7062\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.7140 - val_loss: 0.7050\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.7050 - val_loss: 0.6929\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.7085 - val_loss: 0.6944\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7120 - val_loss: 0.6815\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7030 - val_loss: 0.6755\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6920 - val_loss: 0.6586\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6959 - val_loss: 0.6673\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7020 - val_loss: 0.6712\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 0.6966 - val_loss: 0.6652\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.6904 - val_loss: 0.6635\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6934 - val_loss: 0.6636\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6944 - val_loss: 0.6462\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 123ms/step - loss: 0.8031 - val_loss: 0.7181\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.7347 - val_loss: 0.7075\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7100 - val_loss: 0.7084\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7067 - val_loss: 0.6944\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.6991 - val_loss: 0.6920\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.7252 - val_loss: 0.7020\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.7145 - val_loss: 0.6950\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.6934 - val_loss: 0.6930\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.6836 - val_loss: 0.6641\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.7125 - val_loss: 0.6938\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6838 - val_loss: 0.6821\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.6888 - val_loss: 0.6589\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.6992 - val_loss: 0.6561\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 0.6856 - val_loss: 0.6901\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.6917 - val_loss: 0.6928\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.8240 - val_loss: 0.7031\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.7501 - val_loss: 0.6819\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.7415 - val_loss: 0.7094\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.7260 - val_loss: 0.7395\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 84ms/step - loss: 0.7139 - val_loss: 0.7214\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7123 - val_loss: 0.6903\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7109 - val_loss: 0.6903\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7201 - val_loss: 0.6980\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7037 - val_loss: 0.7158\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7128 - val_loss: 0.7138\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7137 - val_loss: 0.6960\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6982 - val_loss: 0.6946\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6924 - val_loss: 0.6879\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7025 - val_loss: 0.6838\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.7005 - val_loss: 0.6841\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 118ms/step - loss: 0.7901 - val_loss: 0.7108\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 100ms/step - loss: 0.7434 - val_loss: 0.6909\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.7266 - val_loss: 0.7027\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 113ms/step - loss: 0.7060 - val_loss: 0.7097\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7198 - val_loss: 0.7119\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 110ms/step - loss: 0.7012 - val_loss: 0.7127\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.7050 - val_loss: 0.7085\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7069 - val_loss: 0.7039\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6942 - val_loss: 0.6945\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6972 - val_loss: 0.6838\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.7004 - val_loss: 0.6773\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.6983 - val_loss: 0.6696\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 107ms/step - loss: 0.6885 - val_loss: 0.6565\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 582739.7500 - val_loss: 0.6358\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.6878 - val_loss: 0.6864\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 111ms/step - loss: 0.8108 - val_loss: 0.6873\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7513 - val_loss: 0.6939\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.7384 - val_loss: 0.7113\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7349 - val_loss: 0.7408\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 103ms/step - loss: 0.7304 - val_loss: 0.7355\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7324 - val_loss: 0.7289\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.7217 - val_loss: 0.7027\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7330 - val_loss: 0.7101\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 0.7304 - val_loss: 0.7211\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.7486 - val_loss: 0.7543\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7099 - val_loss: 0.7651\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7240 - val_loss: 0.7578\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7015 - val_loss: 0.7408\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.7094 - val_loss: 0.7266\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 81ms/step - loss: 0.7068 - val_loss: 0.7142\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.8008 - val_loss: 0.6882\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.7507 - val_loss: 0.6664\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7356 - val_loss: 0.6951\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7126 - val_loss: 0.7093\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7180 - val_loss: 0.7115\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.7098 - val_loss: 0.7031\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7221 - val_loss: 0.6854\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.6876 - val_loss: 0.6894\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 98ms/step - loss: 0.7052 - val_loss: 0.6920\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6980 - val_loss: 0.6972\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 87ms/step - loss: 0.7051 - val_loss: 0.6961\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 104ms/step - loss: 0.7070 - val_loss: 0.7043\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6953 - val_loss: 0.7067\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6953 - val_loss: 0.6974\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6972 - val_loss: 0.6854\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 112ms/step - loss: 0.8215 - val_loss: 0.7485\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.7777 - val_loss: 0.7441\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.7623 - val_loss: 0.7629\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.7445 - val_loss: 0.7676\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7334 - val_loss: 0.7335\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7356 - val_loss: 0.7227\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.7408 - val_loss: 0.7344\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7256 - val_loss: 0.7388\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.7145 - val_loss: 0.7260\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7167 - val_loss: 0.6956\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.7272 - val_loss: 0.7032\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 99ms/step - loss: 0.7219 - val_loss: 0.7146\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7201 - val_loss: 0.6949\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7119 - val_loss: 0.7411\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.7216 - val_loss: 0.7225\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 122ms/step - loss: 0.8094 - val_loss: 0.7088\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7032 - val_loss: 0.6720\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 100ms/step - loss: 0.7378 - val_loss: 0.6721\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.7263 - val_loss: 0.6966\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.7217 - val_loss: 0.7110\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 1s 105ms/step - loss: 0.7086 - val_loss: 0.7027\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 1s 109ms/step - loss: 0.6921 - val_loss: 0.6822\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 1s 108ms/step - loss: 0.6951 - val_loss: 0.6723\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.6817 - val_loss: 0.6718\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6940 - val_loss: 0.6765\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6943 - val_loss: 0.6774\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6874 - val_loss: 0.6584\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 101ms/step - loss: 0.6897 - val_loss: 0.6708\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 1s 100ms/step - loss: 0.6797 - val_loss: 0.6652\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 1s 106ms/step - loss: 0.6920 - val_loss: 0.6580\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 119ms/step - loss: 0.7879 - val_loss: 0.6984\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7335 - val_loss: 0.6722\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7098 - val_loss: 0.7102\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7318 - val_loss: 0.7179\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7187 - val_loss: 0.7043\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6992 - val_loss: 0.6948\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.7163 - val_loss: 0.6907\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7181 - val_loss: 0.6926\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6978 - val_loss: 0.7137\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.6927 - val_loss: 0.7137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 93ms/step - loss: 0.7023 - val_loss: 0.7126\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 80ms/step - loss: 0.6819 - val_loss: 0.6904\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 11119555584.0000 - val_loss: 0.7072\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7035 - val_loss: 0.7135\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6836 - val_loss: 0.7165\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 117ms/step - loss: 0.8069 - val_loss: 0.7270\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.7950 - val_loss: 0.6885\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.7480 - val_loss: 0.7406\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.7401 - val_loss: 0.7891\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.7504 - val_loss: 0.7733\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 97ms/step - loss: 0.7398 - val_loss: 0.7574\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7165 - val_loss: 0.7386\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7322 - val_loss: 0.7212\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 94ms/step - loss: 0.7238 - val_loss: 0.7349\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7315 - val_loss: 0.7331\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.7104 - val_loss: 0.7383\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 96ms/step - loss: 0.7158 - val_loss: 0.7309\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 1s 102ms/step - loss: 350522240.0000 - val_loss: 0.7464\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 88ms/step - loss: 0.7166 - val_loss: 0.7762\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 82ms/step - loss: 0.7157 - val_loss: 0.7916\n",
      "Epoch 1/15\n",
      "5/5 [==============================] - 1s 116ms/step - loss: 0.7677 - val_loss: 0.6289\n",
      "Epoch 2/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6940 - val_loss: 0.6205\n",
      "Epoch 3/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6831 - val_loss: 0.6417\n",
      "Epoch 4/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6826 - val_loss: 0.6515\n",
      "Epoch 5/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6683 - val_loss: 0.6568\n",
      "Epoch 6/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6529 - val_loss: 0.6457\n",
      "Epoch 7/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6450 - val_loss: 0.6478\n",
      "Epoch 8/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6305 - val_loss: 0.6497\n",
      "Epoch 9/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 8.9980 - val_loss: 0.6686\n",
      "Epoch 10/15\n",
      "5/5 [==============================] - 0s 89ms/step - loss: 0.6164 - val_loss: 0.6769\n",
      "Epoch 11/15\n",
      "5/5 [==============================] - 0s 90ms/step - loss: 0.6450 - val_loss: 0.6813\n",
      "Epoch 12/15\n",
      "5/5 [==============================] - 0s 91ms/step - loss: 0.6424 - val_loss: 0.6832\n",
      "Epoch 13/15\n",
      "5/5 [==============================] - 0s 95ms/step - loss: 0.6513 - val_loss: 0.6838\n",
      "Epoch 14/15\n",
      "5/5 [==============================] - 0s 86ms/step - loss: 0.6407 - val_loss: 0.6833\n",
      "Epoch 15/15\n",
      "5/5 [==============================] - 0s 92ms/step - loss: 0.6424 - val_loss: 0.6826\n",
      "F1-Score: 95% confidence interval 0.000 and 0.437\n",
      "Matthews Corr Coef: 95% confidence interval -0.157 and 0.170\n"
     ]
    }
   ],
   "source": [
    "# Here is an attempt to learn from embeddings only\n",
    "\n",
    "def get_embedding_model():\n",
    "    inputs = tf.keras.layers.Input(shape=(MAX_LEN))\n",
    "    embedding = tf.keras.layers.Embedding(MAX_WORDS, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False)(inputs)\n",
    "    lstm = tf.keras.layers.LSTM(64, activation='relu', kernel_regularizer=L2(1e-6))(embedding)\n",
    "    dropout = tf.keras.layers.Dropout(0.4)(lstm)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=0.0005))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_predict_embedding_model(x_train, x_test, y_train, y_test):\n",
    "    x_train_1, x_test_1 = get_embeddings_model_x_inputs(x_train, x_test)\n",
    "\n",
    "    with local_seed(10):\n",
    "        tf.keras.backend.clear_session()\n",
    "        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=5)\n",
    "\n",
    "        model = get_embedding_model()\n",
    "        result = model.fit(x_train_1,\n",
    "            y_train, \n",
    "            batch_size = 128, \n",
    "            epochs=15,\n",
    "#             callbacks=[es],\n",
    "            validation_data=(x_test_1, y_test),\n",
    "            class_weight=class_weights, verbose=1)\n",
    "\n",
    "    y_pred = model.predict_step(x_test_1).numpy()\n",
    "    y_pred_bool = y_pred > 0.5\n",
    "    return evaluate_model(y_test, y_pred_bool)\n",
    "\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state = 10, test_size=0.2, shuffle=True)\n",
    "# train_and_predict_embedding_model(x_train, x_test, y_train, y_test)\n",
    "f1_output, mmc_output = bootstrap_model_prediction(train_and_predict_embedding_model, 20, stratify=False, model_name=\"Fusion model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting iminent betrayal - Decisions Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing : \n",
    "Features: /\n",
    "\n",
    "\n",
    "Label: Is there a betrayal the season after ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_1[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness']].values, (data_1['betrayal'].values == 1).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95]),\n",
       "                         'max_depth': range(1, 6),\n",
       "                         'n_estimators': range(1, 1001, 100)})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "\n",
    "param_grid = {'n_estimators': range(1, 1001, 100), 'max_depth': range(1, 6), 'learning_rate': np.arange(0.05, 1, 0.05)}\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(x_train_nor, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 101}\n",
      "Best cross-validation score: 0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "# learning_rate = 0.3, max_depth = 1, n_estimators = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 2, 'n_estimators': 21}\n",
      "Best cross-validation score: 0.55\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "\n",
    "param_grid2 = {'n_estimators': range(1, 1001, 10), 'max_depth': range(1, 6)}\n",
    "grid_search2 = GridSearchCV(GradientBoostingClassifier(learning_rate=0.3), param_grid2, cv=5)\n",
    "grid_search2.fit(x_train_nor, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search2.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search2.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-196-ec6b9953c966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_learning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mx_train_nor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_nor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             clf = GradientBoostingClassifier(random_state=0, n_estimators=n_tree, \n\u001b[1;32m     14\u001b[0m                                              \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "range_trees = range(2, 100)\n",
    "range_depth = range(1, 10)\n",
    "range_learning = np.arange(0.1, 1, 0.1)\n",
    "scores = list()\n",
    "features_list = list()\n",
    "\n",
    "for n_tree in range_trees:\n",
    "    for depth in range_depth:\n",
    "        for learning_rate in range_learning:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "            x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "            clf = GradientBoostingClassifier(random_state=0, n_estimators=n_tree, \n",
    "                                             max_depth=depth, \n",
    "                                             learning_rate=learning_rate)\n",
    "            clf.fit(x_train_nor, y_train)\n",
    "            y_pred = clf.predict(x_test_nor)\n",
    "            results = evaluate_model(y_test, y_pred)\n",
    "            results['n_tree'] = n_tree\n",
    "            results['max_depth'] = depth\n",
    "            results['learning_rate'] = learning_rate\n",
    "            results['matthews_score'] = matthews_corr_coef(y_test, y_pred)\n",
    "            scores.append(results)\n",
    "            features_importance = clf.feature_importances_\n",
    "            features = {'sentiment_positive': features_importance[0],\n",
    "                       'sentiment_neutral': features_importance[1],\n",
    "                       'sentiment_negative': features_importance[2],\n",
    "                       'n_requests': features_importance[3],\n",
    "                       'n_words': features_importance[4],\n",
    "                       'n_sentences': features_importance[5],\n",
    "                       'politeness': features_importance[6]}\n",
    "            features_list.append(features)\n",
    "    #print(results)\n",
    "    \n",
    "opti_scores = pd.DataFrame(scores)\n",
    "features = pd.DataFrame(features_list).apply(func=(lambda x: x.mean()), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = opti_scores[opti_scores['f1'] == np.array(opti_scores['f1']).max()][['n_tree', 'max_depth', 'learning_rate']]\n",
    "# Best features: max_depth = 1, learning_rate = 0.6, n_trees = 8\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols = 3, figsize=(16, 5))\n",
    "ax[0].bar(x=range(len(features)), height=features.values)\n",
    "ax[0].set_xticks(range(len(features)))\n",
    "ax[0].set_xticklabels(features.index, rotation = 45)\n",
    "ax[0].set_ylabel('feature importance')\n",
    "\n",
    "\n",
    "ax[1].plot(opti_scores['n_tree'], opti_scores['matthews_score'])\n",
    "ax[1].set_ylabel('matthews score')\n",
    "ax[1].set_xlabel('number of trees')\n",
    "\n",
    "ax[2].plot(opti_scores['n_tree'], opti_scores['f1'])\n",
    "ax[2].set_ylabel('f1 score')\n",
    "ax[2].set_xlabel('number of trees')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "best_clf = GradientBoostingClassifier(random_state=0, \n",
    "                                      n_estimators=8, \n",
    "                                      max_depth=1, \n",
    "                                      learning_rate=0.6)\n",
    "best_clf.fit(x_train_nor, y_train)\n",
    "features_importance = best_clf.feature_importances_\n",
    "y_pred = best_clf.predict(x_test_nor)\n",
    "results = evaluate_model(y_test, y_pred)\n",
    "features = pd.Series({'sentiment_positive': features_importance[0],\n",
    "                       'sentiment_neutral': features_importance[1],\n",
    "                       'sentiment_negative': features_importance[2],\n",
    "                       'n_requests': features_importance[3],\n",
    "                       'n_words': features_importance[4],\n",
    "                       'n_sentences': features_importance[5],\n",
    "                       'politeness': features_importance[6]})\n",
    "\n",
    "fig, ax = plt.subplots(ncols = 1, figsize=(6, 5))\n",
    "ax.bar(x=range(len(features)), height=features.values)\n",
    "ax.set_xticks(range(len(features)))\n",
    "ax.set_xticklabels(features.index, rotation = 45)\n",
    "ax.set_ylabel('feature importance')\n",
    "# Difference in the importance of features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
