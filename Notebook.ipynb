{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import spacy\n",
    "#import gensim.downloader as api\n",
    "\n",
    "#info = api.info()  # show info about available models/datasets\n",
    "#model = api.load(\"glove-twitter-25\")  # download the model and return as object ready for use\n",
    "\n",
    "#nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the Readme here:\n",
    "https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use here the dataset provided by the authors we will create a helper function to extract the features that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seasons</th>\n",
       "      <th>game</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>idx</th>\n",
       "      <th>people</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'season': 1906.5, 'interaction': {'victim': ...</td>\n",
       "      <td>74</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>AT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'season': 1911.5, 'interaction': {'victim': ...</td>\n",
       "      <td>165</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>EG</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             seasons  game  betrayal  idx  \\\n",
       "0  [{'season': 1906.5, 'interaction': {'victim': ...    74      True    0   \n",
       "1  [{'season': 1911.5, 'interaction': {'victim': ...   165     False    1   \n",
       "\n",
       "  people  \n",
       "0     AT  \n",
       "1     EG  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_json(\"diplomacy_data/diplomacy_data.json\")\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_support(entry):\n",
    "    \"\"\"\n",
    "    This function returns the last season of friendship. The code is inspired by the provided code from\n",
    "    the authors\n",
    "    \"\"\"\n",
    "    last_support = None\n",
    "    for season in entry[:-1]:\n",
    "        if 'support' in season['interaction'].values():\n",
    "            last_support = season['season']\n",
    "    return last_support\n",
    "\n",
    "def treat_msg_season(df):\n",
    "    \"\"\"\n",
    "    This function loops over the whole dataset and creates a dictionnary with the set of features for each season \n",
    "    with its associated boolean (betrayal or not )\n",
    "    \"\"\"\n",
    "    data_victim = {'features':[], 'betrayed':[]} # data of the (potential) victim \n",
    "    data_betrayer = {'features':[], 'betrayed':[]} # data of the (potential) betrayer\n",
    "    for i in range(len(df.seasons.values)):\n",
    "        entry = df['seasons'][i] # pick each entry\n",
    "        for j in range(len(entry)): # pick each season\n",
    "            season = entry[j]\n",
    "            tab_vi = []\n",
    "            tab_be = []\n",
    "            if season['season'] <= last_support(entry): # check if the season is below the last season of friendship\n",
    "                tab_vi.append(season['messages']['victim'])\n",
    "                tab_be.append(season['messages']['betrayer'])\n",
    "                if len(tab_be) != 0 and len(tab_vi) != 0: # keep only cases where both players have sent messages\n",
    "                    data_victim['features'].append(tab_vi)\n",
    "                    data_victim['betrayed'].append(df.betrayal.values[i])\n",
    "                    data_betrayer['features'].append(tab_be)   \n",
    "                    data_betrayer['betrayed'].append(df.betrayal.values[i])\n",
    "    return data_victim, data_betrayer\n",
    "\n",
    "data_victim, data_betrayer = treat_msg_season(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(message):\n",
    "    sentiment_positive = message['sentiment']['positive']\n",
    "    sentiment_neutral = message['sentiment']['neutral']\n",
    "    sentiment_negative = message['sentiment']['negative']\n",
    "    n_requests = message['n_requests']\n",
    "    frequent_words = message['frequent_words']\n",
    "    n_words = message['n_words']\n",
    "    politeness = message['politeness']\n",
    "    n_sentences = message['n_sentences']\n",
    "    return {\"sentiment_positive\": sentiment_positive,\n",
    "           \"sentiment_neutral\": sentiment_neutral,\n",
    "           'sentiment_negative': sentiment_negative,\n",
    "           'n_requests': n_requests,\n",
    "           'frequent_words': frequent_words,\n",
    "           'n_words': n_words,\n",
    "           'politeness': politeness,\n",
    "           'n_sentences': n_sentences}\n",
    "    \n",
    "\n",
    "\n",
    "def preprocessing(df):\n",
    "    result = []\n",
    "    for row in df.iterrows():\n",
    "        row = row[1]\n",
    "        betrayal = row['betrayal']\n",
    "        idx = row['idx']\n",
    "        for season in row['seasons']:\n",
    "            s = season['season']\n",
    "                \n",
    "            last_s = last_support(row['seasons'])+0.5 # the betrayal occurs one season after the last support\n",
    "            if s <= last_support(row['seasons']) and len(season['messages']['betrayer']) and len(season['messages']['victim']): # here we also have to consider the last season before betrayal\n",
    "                interaction_victim = season['interaction']['victim']\n",
    "                interaction_betrayer = season ['interaction']['betrayer']\n",
    "                for m_vic in season['messages']['victim']:\n",
    "                    data = to_dict(m_vic)\n",
    "                    data['role'] = 'victim'\n",
    "                    data['season'] = s\n",
    "                    data['betrayal'] = betrayal\n",
    "                    data['season_betrayal'] = last_s\n",
    "                    data['season_before_betrayal'] = (last_s-s)/0.5\n",
    "                    data['idx'] = idx\n",
    "                    result.append(data)\n",
    "                for m_bet in season['messages']['betrayer']:\n",
    "                    data = to_dict(m_bet)\n",
    "                    data['role'] = 'betrayer'\n",
    "                    data['season'] = s\n",
    "                    data['betrayal'] = betrayal\n",
    "                    data['season_betrayal'] = last_s\n",
    "                    data['season_before_betrayal'] = (last_s-s)/0.5\n",
    "                    data['idx'] = idx\n",
    "                    result.append(data)\n",
    "                            \n",
    "    return pd.DataFrame(result).set_index(['idx', 'season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>1906.5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[just, bot, ,, ., take, unit, war, retreat, di...</td>\n",
       "      <td>35</td>\n",
       "      <td>0.367200</td>\n",
       "      <td>2</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[armies, north, the, armies, on, ., your, with...</td>\n",
       "      <td>77</td>\n",
       "      <td>0.932326</td>\n",
       "      <td>6</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[?, going, for, ser, balance, a, to, of, give,...</td>\n",
       "      <td>55</td>\n",
       "      <td>0.983373</td>\n",
       "      <td>4</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>[only, he, alb, ., forced, italy's, is, be, .,...</td>\n",
       "      <td>313</td>\n",
       "      <td>0.957072</td>\n",
       "      <td>19</td>\n",
       "      <td>victim</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1906.5</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>[more, let, keep, we, side, we, don't, to, ., ...</td>\n",
       "      <td>146</td>\n",
       "      <td>0.832023</td>\n",
       "      <td>9</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "idx season                                                              \n",
       "0   1906.5                   0                  0                   2   \n",
       "    1906.5                   1                  1                   4   \n",
       "    1906.5                   1                  2                   1   \n",
       "    1906.5                   4                  2                  13   \n",
       "    1906.5                   1                  3                   5   \n",
       "\n",
       "            n_requests                                     frequent_words  \\\n",
       "idx season                                                                  \n",
       "0   1906.5           1  [just, bot, ,, ., take, unit, war, retreat, di...   \n",
       "    1906.5           2  [armies, north, the, armies, on, ., your, with...   \n",
       "    1906.5           2  [?, going, for, ser, balance, a, to, of, give,...   \n",
       "    1906.5           8  [only, he, alb, ., forced, italy's, is, be, .,...   \n",
       "    1906.5           7  [more, let, keep, we, side, we, don't, to, ., ...   \n",
       "\n",
       "            n_words  politeness  n_sentences      role  betrayal  \\\n",
       "idx season                                                         \n",
       "0   1906.5       35    0.367200            2    victim      True   \n",
       "    1906.5       77    0.932326            6    victim      True   \n",
       "    1906.5       55    0.983373            4    victim      True   \n",
       "    1906.5      313    0.957072           19    victim      True   \n",
       "    1906.5      146    0.832023            9  betrayer      True   \n",
       "\n",
       "            season_betrayal  season_before_betrayal  \n",
       "idx season                                           \n",
       "0   1906.5           1909.5                     6.0  \n",
       "    1906.5           1909.5                     6.0  \n",
       "    1906.5           1909.5                     6.0  \n",
       "    1906.5           1909.5                     6.0  \n",
       "    1906.5           1909.5                     6.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = preprocessing(data)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In each season, potential betrayers send in average 1.627498001598721, with a maximum of 38 messages\n",
      "In each season, potential victims send in average 1.515587529976019, with a maximum of 28 messages\n"
     ]
    }
   ],
   "source": [
    "def get_nb_msg(data):\n",
    "    \"\"\"\n",
    "    Get the mean number of messages sent per season\n",
    "    \"\"\"\n",
    "    tab = []\n",
    "    for features in data[\"features\"]:\n",
    "        tab.append(len(features[0]))\n",
    "    return tab\n",
    "\n",
    "print(\"In each season, potential betrayers send in average {}, with a maximum of {} messages\".format(np.mean(get_nb_msg(data_betrayer)), np.max(get_nb_msg(data_betrayer))))\n",
    "print(\"In each season, potential victims send in average {}, with a maximum of {} messages\".format(np.mean(get_nb_msg(data_victim)), np.max(get_nb_msg(data_victim))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexicon_words(entry):\n",
    "    \"\"\"\n",
    "    get the set of lexicon words for each entry of the dataset\n",
    "    1 entry = 1 row = 1 set of messages\n",
    "    Can be improved\n",
    "    \"\"\"\n",
    "    for entries in entry[0]: #loop over the messages\n",
    "        # get the lexicon words\n",
    "        di_words = entries[\"lexicon_words\"]\n",
    "        tab_words = []\n",
    "        for key in di_words:\n",
    "            tab = di_words[key]\n",
    "            for words in tab:\n",
    "                word = words.split(' ')\n",
    "                for w in word:\n",
    "                    if w not in tab_words:\n",
    "                        tab_words.append(w)\n",
    "    return tab_words\n",
    "\n",
    "test = get_lexicon_words(data_victim[\"features\"][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "season_before_betrayal\n",
       "1.0    343\n",
       "2.0    360\n",
       "3.0    314\n",
       "4.0    277\n",
       "5.0    231\n",
       "6.0    156\n",
       "7.0     94\n",
       "8.0     78\n",
       "9.0     42\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df[\"betrayal\"]==True) & (df[\"role\"] == \"betrayer\")].reset_index().groupby('season_before_betrayal').count()['idx']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 367 instances of 1 season before betrayal, 379 instances of 2 seasons before betrayal etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting iminent betrayal - Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we consider only messages that have been exchanged one season before the betrayal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[would, on, other, france, i, that, to, think,...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.951652</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, the, the, ,, game, that, unless, on, wa...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.867535</td>\n",
       "      <td>2</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[france, for, see, now, would, did, just, what...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.464116</td>\n",
       "      <td>1</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[bla, not, of, always, a, if, goes, take, ,, t...</td>\n",
       "      <td>134</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>6</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[about, to, aeg, was, now, anyway, has, gre, b...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.218056</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[war, after, and, which, that, into, may, you,...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.784303</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[another, game, ., we, i, ., think, 6, have, ....</td>\n",
       "      <td>18</td>\n",
       "      <td>0.690164</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[vie, into, and, ., you, tun, also, ahead, sup...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.600676</td>\n",
       "      <td>2</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[and, through, ., sounds, enter, to, i'm, move...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.852749</td>\n",
       "      <td>4</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[on, talk, ., write, ., and, if, more, perfect...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  season  sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "0    0  1909.0                   0                  1                   2   \n",
       "1    0  1909.0                   0                  0                   2   \n",
       "2    0  1909.0                   0                  1                   0   \n",
       "3    3  1902.5                   0                  2                   4   \n",
       "4    3  1902.5                   1                  0                   2   \n",
       "5    4  1911.0                   0                  2                   1   \n",
       "6    4  1911.0                   0                  3                   0   \n",
       "7    4  1911.0                   1                  1                   0   \n",
       "8    5  1906.0                   3                  0                   1   \n",
       "9    8  1906.0                   1                  2                   0   \n",
       "\n",
       "   n_requests                                     frequent_words  n_words  \\\n",
       "0           2  [would, on, other, france, i, that, to, think,...       42   \n",
       "1           1  [know, the, the, ,, game, that, unless, on, wa...       51   \n",
       "2           1  [france, for, see, now, would, did, just, what...       14   \n",
       "3           4  [bla, not, of, always, a, if, goes, take, ,, t...      134   \n",
       "4           2  [about, to, aeg, was, now, anyway, has, gre, b...       62   \n",
       "5           2  [war, after, and, which, that, into, may, you,...       44   \n",
       "6           0  [another, game, ., we, i, ., think, 6, have, ....       18   \n",
       "7           0  [vie, into, and, ., you, tun, also, ahead, sup...       17   \n",
       "8           1  [and, through, ., sounds, enter, to, i'm, move...       33   \n",
       "9           1  [on, talk, ., write, ., and, if, more, perfect...       20   \n",
       "\n",
       "   politeness  n_sentences      role  betrayal  season_betrayal  \\\n",
       "0    0.951652            3  betrayer      True           1909.5   \n",
       "1    0.867535            2  betrayer      True           1909.5   \n",
       "2    0.464116            1  betrayer      True           1909.5   \n",
       "3    0.855036            6  betrayer     False           1903.0   \n",
       "4    0.218056            3  betrayer     False           1903.0   \n",
       "5    0.784303            3  betrayer     False           1911.5   \n",
       "6    0.690164            3  betrayer     False           1911.5   \n",
       "7    0.600676            2  betrayer     False           1911.5   \n",
       "8    0.852749            4  betrayer      True           1906.5   \n",
       "9    0.480527            3  betrayer      True           1906.5   \n",
       "\n",
       "   season_before_betrayal  \n",
       "0                     1.0  \n",
       "1                     1.0  \n",
       "2                     1.0  \n",
       "3                     1.0  \n",
       "4                     1.0  \n",
       "5                     1.0  \n",
       "6                     1.0  \n",
       "7                     1.0  \n",
       "8                     1.0  \n",
       "9                     1.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = df[(df[\"season_before_betrayal\"]==1) & (df[\"role\"] == \"betrayer\")].reset_index().copy()\n",
    "data_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>season</th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>frequent_words</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>role</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>season_betrayal</th>\n",
       "      <th>season_before_betrayal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[would, on, other, france, i, that, to, think,...</td>\n",
       "      <td>42</td>\n",
       "      <td>0.951652</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[know, the, the, ,, game, that, unless, on, wa...</td>\n",
       "      <td>51</td>\n",
       "      <td>0.867535</td>\n",
       "      <td>2</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1909.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[france, for, see, now, would, did, just, what...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.464116</td>\n",
       "      <td>1</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1909.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[bla, not, of, always, a, if, goes, take, ,, t...</td>\n",
       "      <td>134</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>6</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>1902.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[about, to, aeg, was, now, anyway, has, gre, b...</td>\n",
       "      <td>62</td>\n",
       "      <td>0.218056</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1903.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[war, after, and, which, that, into, may, you,...</td>\n",
       "      <td>44</td>\n",
       "      <td>0.784303</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[another, game, ., we, i, ., think, 6, have, ....</td>\n",
       "      <td>18</td>\n",
       "      <td>0.690164</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>1911.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[vie, into, and, ., you, tun, also, ahead, sup...</td>\n",
       "      <td>17</td>\n",
       "      <td>0.600676</td>\n",
       "      <td>2</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>False</td>\n",
       "      <td>1911.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[and, through, ., sounds, enter, to, i'm, move...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.852749</td>\n",
       "      <td>4</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[on, talk, ., write, ., and, if, more, perfect...</td>\n",
       "      <td>20</td>\n",
       "      <td>0.480527</td>\n",
       "      <td>3</td>\n",
       "      <td>betrayer</td>\n",
       "      <td>True</td>\n",
       "      <td>1906.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  season  sentiment_positive  sentiment_neutral  sentiment_negative  \\\n",
       "0    0  1909.0                   0                  1                   2   \n",
       "1    0  1909.0                   0                  0                   2   \n",
       "2    0  1909.0                   0                  1                   0   \n",
       "3    3  1902.5                   0                  2                   4   \n",
       "4    3  1902.5                   1                  0                   2   \n",
       "5    4  1911.0                   0                  2                   1   \n",
       "6    4  1911.0                   0                  3                   0   \n",
       "7    4  1911.0                   1                  1                   0   \n",
       "8    5  1906.0                   3                  0                   1   \n",
       "9    8  1906.0                   1                  2                   0   \n",
       "\n",
       "   n_requests                                     frequent_words  n_words  \\\n",
       "0           2  [would, on, other, france, i, that, to, think,...       42   \n",
       "1           1  [know, the, the, ,, game, that, unless, on, wa...       51   \n",
       "2           1  [france, for, see, now, would, did, just, what...       14   \n",
       "3           4  [bla, not, of, always, a, if, goes, take, ,, t...      134   \n",
       "4           2  [about, to, aeg, was, now, anyway, has, gre, b...       62   \n",
       "5           2  [war, after, and, which, that, into, may, you,...       44   \n",
       "6           0  [another, game, ., we, i, ., think, 6, have, ....       18   \n",
       "7           0  [vie, into, and, ., you, tun, also, ahead, sup...       17   \n",
       "8           1  [and, through, ., sounds, enter, to, i'm, move...       33   \n",
       "9           1  [on, talk, ., write, ., and, if, more, perfect...       20   \n",
       "\n",
       "   politeness  n_sentences      role  betrayal  season_betrayal  \\\n",
       "0    0.951652            3  betrayer      True           1909.5   \n",
       "1    0.867535            2  betrayer      True           1909.5   \n",
       "2    0.464116            1  betrayer      True           1909.5   \n",
       "3    0.855036            6  betrayer     False           1903.0   \n",
       "4    0.218056            3  betrayer     False           1903.0   \n",
       "5    0.784303            3  betrayer     False           1911.5   \n",
       "6    0.690164            3  betrayer     False           1911.5   \n",
       "7    0.600676            2  betrayer     False           1911.5   \n",
       "8    0.852749            4  betrayer      True           1906.5   \n",
       "9    0.480527            3  betrayer      True           1906.5   \n",
       "\n",
       "   season_before_betrayal  \n",
       "0                     1.0  \n",
       "1                     1.0  \n",
       "2                     1.0  \n",
       "3                     1.0  \n",
       "4                     1.0  \n",
       "5                     1.0  \n",
       "6                     1.0  \n",
       "7                     1.0  \n",
       "8                     1.0  \n",
       "9                     1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_1[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', 'n_sentences', 'politeness']].values, 1*data_1['betrayal'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "def normalize(x):\n",
    "    x_temp = x.copy()\n",
    "    for i in range(len(x[0, :])-1):\n",
    "        m = np.mean(x[:, i])\n",
    "        s = np.std(x[:, i])\n",
    "        x_temp[:, i] -= m\n",
    "        x_temp[:, i] /= s\n",
    "    return x_temp\n",
    "\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An intuition for finding the best model / configuration of features for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def log_reg_pipeline(data, features, normalize_=True, print_=True):\n",
    "    X, Y = data[features].values, 1*data['betrayal'].values\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "    if normalize_:\n",
    "        x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "        clf = LogisticRegression(random_state=0).fit(x_train_nor, y_train)\n",
    "        score = clf.score(x_test_nor, y_test)\n",
    "        if print_:\n",
    "            print(\"score on the test set : {}\".format(score))\n",
    "        return clf, score\n",
    "    else:\n",
    "        clf = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
    "        score = clf.score(x_test, y_test)\n",
    "        if print_:\n",
    "            print(\"score on the test set : {}\".format(score))\n",
    "        return clf, score\n",
    "\n",
    "features = [\n",
    "            ['politeness'],\n",
    "            ['n_words', 'politeness'],\n",
    "            ['n_requests', 'n_words', 'politeness'],\n",
    "            ['sentiment_positive', 'n_requests', 'n_words', 'politeness'],\n",
    "            ['sentiment_positive', 'sentiment_neutral', 'n_requests', 'n_words', 'politeness'],\n",
    "           ['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', 'politeness']]\n",
    "\n",
    "ax1 = []\n",
    "ax2 = []\n",
    "ax3 = []\n",
    "\n",
    "for feature in features:\n",
    "    _, score = log_reg_pipeline(data_1, feature, False, False)\n",
    "    _, score_nor = log_reg_pipeline(data_1, feature, True, False)\n",
    "    ax1.append(len(feature))\n",
    "    ax2.append(score)\n",
    "    ax3.append(score_nor)\n",
    "    \n",
    "_ = plt.plot(ax1, ax2)\n",
    "_ = plt.plot(ax1, ax3)\n",
    "_ = plt.legend(['not normalized', 'normalized'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The feed forward model + some preprocessing to understand better the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = df.copy()\n",
    "\n",
    "# Now train just for the betrayer role for changes, although we could add data for both?\n",
    "features_data = features_data[features_data['role'] == 'betrayer']\n",
    "features_data = features_data[features_data['betrayal'] == True]\n",
    "\n",
    "features_data = features_data.drop(columns=['frequent_words', 'season_betrayal', 'role', 'betrayal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggreagted_features_per_season = features_data.groupby(['idx', 'season'], as_index=False).aggregate({\n",
    "    'sentiment_positive': 'mean',\n",
    "    'sentiment_neutral': 'mean',\n",
    "    'sentiment_negative': 'mean',\n",
    "    'n_requests': 'mean',\n",
    "    'n_words': 'sum',\n",
    "    'politeness': 'mean',\n",
    "    'n_sentences': 'sum',\n",
    "    'season_before_betrayal': 'min'\n",
    "})\n",
    "\n",
    "X = aggreagted_features_per_season[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative',\n",
    "       'n_requests', 'n_words', 'politeness', 'n_sentences']]\n",
    "Y = (aggreagted_features_per_season['season_before_betrayal'] == 1.0).values.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_positive</th>\n",
       "      <th>sentiment_neutral</th>\n",
       "      <th>sentiment_negative</th>\n",
       "      <th>n_requests</th>\n",
       "      <th>n_words</th>\n",
       "      <th>politeness</th>\n",
       "      <th>n_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>489</td>\n",
       "      <td>0.803328</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>1.285714</td>\n",
       "      <td>280</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>333</td>\n",
       "      <td>0.982703</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>3.200000</td>\n",
       "      <td>449</td>\n",
       "      <td>0.748802</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>78</td>\n",
       "      <td>0.899161</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment_positive  sentiment_neutral  sentiment_negative  n_requests  \\\n",
       "0            1.333333           1.333333            1.500000    3.666667   \n",
       "1            0.142857           0.857143            1.285714    1.285714   \n",
       "2            2.000000           2.500000            2.000000    5.500000   \n",
       "3            1.800000           0.800000            2.200000    3.200000   \n",
       "4            1.000000           1.000000            1.000000    2.000000   \n",
       "\n",
       "   n_words  politeness  n_sentences  \n",
       "0      489    0.803328           25  \n",
       "1      280    0.560083           16  \n",
       "2      333    0.982703           13  \n",
       "3      449    0.748802           24  \n",
       "4       78    0.899161            6  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(663, 7)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19457013574660634"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check imbalance of labels\n",
    "np.sum(Y == 1)/(Y.shape[0]) # ~18%; idk why not ~14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, shuffle=True, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train) # use the mean, std of the training to standardize the test\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-08e31a9298de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregularizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mL1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1L2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import L1, L2, L1L2\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "# Please note that these are computed per batch, not on entire validation\n",
    "# and might be misleading\n",
    "METRICS = [\n",
    "      metrics.TruePositives(name='tp'),\n",
    "      metrics.FalsePositives(name='fp'),\n",
    "      metrics.TrueNegatives(name='tn'),\n",
    "      metrics.FalseNegatives(name='fn'),\n",
    "      metrics.BinaryAccuracy(name='accuracy'),\n",
    "      metrics.Precision(name='precision'),\n",
    "      metrics.Recall(name='recall'),\n",
    "      metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "FEATURES_NUM = X.shape[-1]\n",
    "\n",
    "def get_feed_forward_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=512, activation='relu', kernel_regularizer=L1L2(1e-6), input_shape=(FEATURES_NUM,)))\n",
    "    model.add(Dense(units=256, activation='relu', kernel_regularizer=L1L2(1e-6)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    opt = keras.optimizers.Adam(learning_rate=0.00001)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=METRICS)\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.6207865168539326, 1: 2.5697674418604652}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To address imbalance, compute the weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "weights = class_weight.compute_class_weight(class_weight=\"balanced\", classes=np.unique(Y), y=Y)\n",
    "class_weights = {0: weights[0], 1: weights[1]}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-56bfea8cda20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model_tmp.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model_tmp.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "model = get_feed_forward_model()\n",
    "result = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=(x_test, y_test), \n",
    "    callbacks = [es, mc],\n",
    "    class_weight=class_weights, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-de9b57111919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(result.history['loss'], label='train')\n",
    "pyplot.plot(result.history['val_loss'], label='test')\n",
    "\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-53fd75de9dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# This results are cool, but they are much lower when we will do bootstrapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0my_pred_bool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# TODO: apply bootstrapping \n",
    "def matthews_corr_coef(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TP, FP, FN, TN = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "    tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    accuracy = np.sum(y_pred == y_true) / n\n",
    "    \n",
    "    mmc = matthews_corr_coef(y_true, y_pred)\n",
    "    \n",
    "    return {'f1': np.round(f1, decimals=3),\n",
    "            'mmc': np.round(mmc, decimals=6),\n",
    "            'acc': np.round(accuracy, decimals=3),\n",
    "            'precision': np.round(precision, decimals=3),\n",
    "            'recall': np.round(recall, decimals=3),\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'tn': tn,\n",
    "            'fn': fn\n",
    "           }\n",
    "\n",
    "# This results are cool, but they are much lower when we will do bootstrapping\n",
    "model.load_weights('best_model.h5')\n",
    "y_pred = model.predict(x_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "y_pred_bool = y_pred > 0.5\n",
    "evaluate_model(y_test, y_pred_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short attempt to the time series idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 3\n",
    "\n",
    "def split_sequence(X_data, y_data, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(X_data)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(X_data)-1:\n",
    "            break\n",
    "        seq_x = X_data[i:end_ix]\n",
    "        seq_y = y_data[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xp = scaler.fit_transform(X)\n",
    "x_seq, y_seq = split_sequence(Xp, Y, TIMESTEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_seq, y_seq, test_size=0.10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-eb7532acdbe9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mmc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model_tmp.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "def get_vanilla_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.LSTM(30, activation='relu', kernel_regularizer=keras.regularizers.L1L2(1e-6), input_shape=(TIMESTEPS, FEATURES_NUM)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    opt = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=METRICS)\n",
    "\n",
    "    return model\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model_tmp.h5', monitor='val_loss', mode='min', save_best_only=True)\n",
    "\n",
    "model = get_vanilla_lstm_model()\n",
    "result = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=(x_test, y_test), \n",
    "    callbacks = [es, mc],\n",
    "    class_weight=class_weights, verbose=1)\n",
    "\n",
    "model.load_weights('best_model_tmp.h5')\n",
    "y_pred = model.predict(x_test, batch_size=BATCH_SIZE, verbose=1)\n",
    "y_pred_bool = y_pred > 0.5\n",
    "evaluate_model(y_test, y_pred_bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting iminent betrayal - Decisions Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing : \n",
    "Features: /\n",
    "\n",
    "\n",
    "Label: Is there a betrayal the season after ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_1[['sentiment_positive', 'sentiment_neutral', 'sentiment_negative', 'n_requests', 'n_words', \n",
    "               'n_sentences', 'politeness']].values, (data_1['betrayal'].values == 1).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingClassifier(),\n",
       "             param_grid={'learning_rate': array([0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55,\n",
       "       0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95]),\n",
       "                         'max_depth': range(1, 6),\n",
       "                         'n_estimators': range(1, 1001, 100)})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "\n",
    "param_grid = {'n_estimators': range(1, 1001, 100), 'max_depth': range(1, 6), 'learning_rate': np.arange(0.05, 1, 0.05)}\n",
    "grid_search = GridSearchCV(GradientBoostingClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(x_train_nor, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.3, 'max_depth': 1, 'n_estimators': 101}\n",
      "Best cross-validation score: 0.55\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "# learning_rate = 0.3, max_depth = 1, n_estimators = 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 2, 'n_estimators': 21}\n",
      "Best cross-validation score: 0.55\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "\n",
    "\n",
    "param_grid2 = {'n_estimators': range(1, 1001, 10), 'max_depth': range(1, 6)}\n",
    "grid_search2 = GridSearchCV(GradientBoostingClassifier(learning_rate=0.3), param_grid2, cv=5)\n",
    "grid_search2.fit(x_train_nor, y_train)\n",
    "print(\"Best parameters: {}\".format(grid_search2.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search2.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n",
      "<ipython-input-24-53fd75de9dac>:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return (TN*TP - FP*FN)/np.sqrt((TN+FN)*(FP+TP)*(TN+FP)*(FN+TP))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-ec6b9953c966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m                                              \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                              learning_rate=learning_rate)\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_nor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test_nor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[0;32m~/Anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[0;32m~/Anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             loss.update_terminal_regions(\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 sample_mask, learning_rate=self.learning_rate, k=k)\n",
      "\u001b[0;32m~/Anaconda3/envs/ada/lib/python3.8/site-packages/sklearn/ensemble/_gb_losses.py\u001b[0m in \u001b[0;36mupdate_terminal_regions\u001b[0;34m(self, tree, X, y, residual, raw_predictions, sample_weight, sample_mask, learning_rate, k)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# update each leaf (= perform line search)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mleaf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren_left\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTREE_LEAF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             self._update_terminal_region(tree, masked_terminal_regions,\n\u001b[0m\u001b[1;32m    113\u001b[0m                                          \u001b[0mleaf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                                          raw_predictions[:, k], sample_weight)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "range_trees = range(2, 100)\n",
    "range_depth = range(1, 10)\n",
    "range_learning = np.arange(0.1, 1, 0.1)\n",
    "scores = list()\n",
    "features_list = list()\n",
    "\n",
    "for n_tree in range_trees:\n",
    "    for depth in range_depth:\n",
    "        for learning_rate in range_learning:\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "            x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "            clf = GradientBoostingClassifier(random_state=0, n_estimators=n_tree, \n",
    "                                             max_depth=depth, \n",
    "                                             learning_rate=learning_rate)\n",
    "            clf.fit(x_train_nor, y_train)\n",
    "            y_pred = clf.predict(x_test_nor)\n",
    "            results = evaluate_model(y_test, y_pred)\n",
    "            results['n_tree'] = n_tree\n",
    "            results['max_depth'] = depth\n",
    "            results['learning_rate'] = learning_rate\n",
    "            results['matthews_score'] = matthews_corr_coef(y_test, y_pred)\n",
    "            scores.append(results)\n",
    "            features_importance = clf.feature_importances_\n",
    "            features = {'sentiment_positive': features_importance[0],\n",
    "                       'sentiment_neutral': features_importance[1],\n",
    "                       'sentiment_negative': features_importance[2],\n",
    "                       'n_requests': features_importance[3],\n",
    "                       'n_words': features_importance[4],\n",
    "                       'n_sentences': features_importance[5],\n",
    "                       'politeness': features_importance[6]}\n",
    "            features_list.append(features)\n",
    "    #print(results)\n",
    "    \n",
    "opti_scores = pd.DataFrame(scores)\n",
    "features = pd.DataFrame(features_list).apply(func=(lambda x: x.mean()), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = opti_scores[opti_scores['f1'] == np.array(opti_scores['f1']).max()][['n_tree', 'max_depth', 'learning_rate']]\n",
    "# Best features: max_depth = 1, learning_rate = 0.6, n_trees = 8\n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols = 3, figsize=(16, 5))\n",
    "ax[0].bar(x=range(len(features)), height=features.values)\n",
    "ax[0].set_xticks(range(len(features)))\n",
    "ax[0].set_xticklabels(features.index, rotation = 45)\n",
    "ax[0].set_ylabel('feature importance')\n",
    "\n",
    "\n",
    "ax[1].plot(opti_scores['n_tree'], opti_scores['matthews_score'])\n",
    "ax[1].set_ylabel('matthews score')\n",
    "ax[1].set_xlabel('number of trees')\n",
    "\n",
    "ax[2].plot(opti_scores['n_tree'], opti_scores['f1'])\n",
    "ax[2].set_ylabel('f1 score')\n",
    "ax[2].set_xlabel('number of trees')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=42)\n",
    "x_train_nor, x_test_nor = normalize(x_train), normalize(x_test)\n",
    "best_clf = GradientBoostingClassifier(random_state=0, \n",
    "                                      n_estimators=8, \n",
    "                                      max_depth=1, \n",
    "                                      learning_rate=0.6)\n",
    "best_clf.fit(x_train_nor, y_train)\n",
    "features_importance = best_clf.feature_importances_\n",
    "y_pred = best_clf.predict(x_test_nor)\n",
    "results = evaluate_model(y_test, y_pred)\n",
    "features = pd.Series({'sentiment_positive': features_importance[0],\n",
    "                       'sentiment_neutral': features_importance[1],\n",
    "                       'sentiment_negative': features_importance[2],\n",
    "                       'n_requests': features_importance[3],\n",
    "                       'n_words': features_importance[4],\n",
    "                       'n_sentences': features_importance[5],\n",
    "                       'politeness': features_importance[6]})\n",
    "\n",
    "fig, ax = plt.subplots(ncols = 1, figsize=(6, 5))\n",
    "ax.bar(x=range(len(features)), height=features.values)\n",
    "ax.set_xticks(range(len(features)))\n",
    "ax.set_xticklabels(features.index, rotation = 45)\n",
    "ax.set_ylabel('feature importance')\n",
    "# Difference in the importance of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"helpers/stopwords.txt\", encoding='utf-8') as h:\n",
    "    stopwords = h.read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy & Glove word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the spacy library to compute the embeddings. We have to try also with Glove and Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def get_word_embedding(word):\n",
    "    return nlp(word).vector\n",
    "\n",
    "def get_word_embedding_model(model, word):\n",
    "    return model[word]\n",
    "\n",
    "print(\"Word :{} , embedding : {}\".format(test[1], get_word_embedding_model(model, test[1])))\n",
    "#model.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
